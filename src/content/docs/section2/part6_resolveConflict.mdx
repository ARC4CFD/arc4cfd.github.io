---
title: 'Running simulations on HPC'
---

import Box from '../../../components/Box.astro';
import Caption from '../../../components/Caption.astro';
import Option from '../../../components/Option.astro';
import CustomAside from '../../../components/CustomAside.astro';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import CodeFetch from '../../../components/CodeFetch.astro';
import MultipleChoice from '../../../components/MultipleChoice.astro';
import { YouTube } from '@astro-community/astro-embed-youtube';

:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Organize and plan simulation files on the cluster
2. Run the simulation and optimize
3. Carry out a runtime analysis of flow parameters
:::
<CustomAside icon="pen" title="Time to complete: 45 min" colour="green"></CustomAside>

We are now ready to start a large-scale CFD simulation. We have generated the mesh, setup the simulation, and run scaling tests.  The purpose of this section is to standardize the workflow of organizing, running, and monitoring a large-scale CFD simulations on a remote HPC system and to provide best-practice tips.
![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_workflow_PROC.png "Process simulations of the CFD simulation")

:::caution[Disclaimer]
1. All examples will be carried out on Graham, but the approach is generalizable to any  HPC system
2. The workflow is NOT *set in stone*, but rather  *suggestions* to facilitate HPC usage
3. The examples will be carried out in  both OpenFOAM and SU2; students can toggle between OpenFOAM and SU2 as shown below
<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="Click for OpenFOAM">
    OpenFOAM commands
    </TabItem>
    <TabItem label="Click for SU2">
    SU2 commands
    </TabItem>
</Tabs>
</Box>
:::

## Remote files on the cluster
Prior to running a simulation, it is good practice to organize the file system on the remote cluster. Although this step may be timely to implement, it will save time in the long run. The question now is: **how do we organize our simulation files?**

{/*
If you are following along with the course, you probably have already cloned all the example files from the GitHub repository onto the Cluster. If not please go back and check [section 2.1](https://arc4cfd.github.io/section2/part1/). */}


To answer this question, let's first figure out what options we have. Upon logging into Graham, type:
```bash
[username@gra-login1 ~]$ diskusage_report
```
This command will check available disk space and the current disk utilization on our personal and group profiles. The output will look something like:
```bash
[username@gra-login1 ~]$ 
                             Description                 Space             # of files
                     /home (user username)              23G/50G             112/500k
                  /scratch (user username)            6633G/20T            14k/1000k
                 /project (group username)              0/2048k               0/1025
               /project (group def-piname)           292M/1000G             294/505k
```
Where `username` refers to your personal space, while `piname` to your group (or principal investigator) profile. 

`/home` has a capacity of 50GB and, as we mentioned earlier, is not the recommended place to store simulation results or data. `/home` is suitable for code development and version control. `/project (group rrg-piname-ac)` (in this case) is the largest directory on the cluster, it's linked to your principal investigator account and meant for longer-term storage. Finally, `/scratch` is the second largest directory, it's connected to the single user, and as mentioned in [section 2.1](https://arc4cfd.github.io/section2/part1/), it's the right place to **set up and run** your simulations.

:::danger[Reminder Warning]
Important files must be copied off `/scratch` regularly since they are not backed up and older files are subject to purging!
:::

Following this approach, one can clone from the course [GitHub repository](https://github.com/ARC4CFD/arc4cfd/tree/master) both the OpenFOAM and SU2 BFS examples to the remote `/home` into their respective case directories:

<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="/home/bfs_openfoam">
    ```bash
    [username@gra-login1 ~]$ ls
    README              case                movie.ogv           run.sh
    bfs_Umag.pvsm       mesh                run_jobscript.sh
    ```
    </TabItem>
    <TabItem label="/home/bfs_su2">
    ```bash
    [username@gra-login1 ~]$ ls
    Coarse                                       README
    Fine                                         Scaling
    Intermediate                                 Turbulent-flow-over-Backward-facing-step.pdf
    ```
    </TabItem>
</Tabs>
</Box>

### Create the Run directory
Now that you have cloned the repository to your `/home` directory, we can make any modifications . Any modifications you want to do on the source should happen in `/home`, as it's your own personal space, and nothing will be purged from here.

:::caution[Caution]
`/home` is not the place **to run** your simulation. Depending on the size of your mesh, and the number of snapshots saved, you will reach the **quota limit** of 50GB very soon. 
:::

For this reason, once you are happy with the changes implemented on the source code, you should **copy** the code into a **run directory** in `/scratch`. In this case no changes were required in the source files, therefore we can copy them directly:
<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="/scratch/01_BFS_openFOAM">
    ```bash
    [username@gra-login1 ~/home/bfs_openfoam]$ cp -r * ./scratch/01_BFS_openFOAM
    ```
    </TabItem>
    <TabItem label="/scratch/02_BFS_SU2">
    ```bash
    [username@gra-login1 ~/home/bfs_su2]$ cp -r ./Coarse/ ./scratch/02_BFS_SU2
    ```
    </TabItem>
</Tabs>
</Box>

Don't underestimate the importance of **naming conventions** for files and directories. We recommend to find your own preferred system and be consistent with it. **At this point we are ready to setup the simulation.**


### Setting up simulation parameters
All the simulation files have already been prepared, and here we highlight only the most important steps to follow before running the simulation:

1. **Mesh size**: the first and most important choice is the **number of points**. We prepared the case so that students can run several **meshes**. Here, however, for simplicity's sake, we only follow the example of the coarsest mesh of $237000$ grid points (named `bfs_200k_DDES` in OpenFoam, and `Coarse` in SU2 examples).

2. **Time step size**: the choice of the grid size directly impacts the maximum time step size one can use due to the CFL stability condition ([section 2.3](https://arc4cfd.github.io/section2/part3/)). For the chosen case (of $\approx 200000$ points) the CFL stability condition requires: $\Delta_t=10^{-4}$.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change deltaT to 1e-4;
    ```
</details>
<details>
    <summary>SU2</summary>
    ```bash
    vim Coarse/Backstep_str_config.cfg
    # change TIME_STEP (line 118) 
    ```
</details>

3. **Domain decomposition**: after performing the scaling test for a given mesh ([section 2.5](https://arc4cfd.github.io/section2/part5/)), we know how many processors we should use to optimize the CFD workflow.
<details>
    <summary>OpenFOAM</summary>
    The user should modify the `numberOfSubdomains` entry in the `case/system/decomposeParDict` file. In this example we use 64 processors.
    ```bash
    vim case/system/decomposeParDict
    # change numberOfSubdomains to 64;
    ```
</details>
<details>
    <summary>SU2</summary>
    **No need to specify this *a priori* in SU2**. During execution the user will decide how many processors to use for the calculation.
</details>

4. **Simulation End Time**: especially when turbulence and/or geometric non-homogeneities are present, it's important to run the simulation long enough to let the flow properly develop in the computational domain. This phase is usually referred to as **the transient** phase. Once the flow is fully developed, a **steady state** is reached and the flow is referred to as **statistically stationary**. As mentioned in [section 2.3](https://arc4cfd.github.io/section2/part3/) estimating the time required to reach a steady-state is very difficult and is case-dependent. A reasonably good measure to get a rough estimate is the flow-through time as described in [section 2.2](https://arc4cfd.github.io/section2/part2/). **In this example we choose the End Time to correspond to $\approx 10$ flow-through times**.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change endTime to 0.5;
    ```
</details>
<details>
    <summary>SU2</summary>
    ```bash
    vim Coarse/Backstep_str_config.cfg
    # change TIME_ITER (line 120) to 5000
    ```
</details>

5. **Snapshot time interval**: once again, when turbulence governs the physics of the flow we use statistical methods to process and analyze the results. For this reason one should collect a large sample of **flow realization** or snapshots in order to perform post-processing analysis and compute flow statistics. The interval between two successive snapshots should not be too large (as they will be uncorrelated) nor too small (as they will be too correlated). One can always refer to the LETOT and choose a percentage of that as the sampling interval.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change writeInterval to 20;
    ```
</details>
<details>
    <summary>SU2</summary>
    ```bash
    vim Coarse/Backstep_str_config.cfg
    # change OUTPUT_WRT_FREQ to OUTPUT_WRT_FREQ = 4800, 20
    # this will make sure that restart files are written before 
    # the simulation end time is reached, and snapshots are saved every 20 steps.
    ```
</details>

After all flow and simulation parameters have been set, **we are now ready to run the simulation**. 

## Run a large-scale CFD simulation
As previously seen in [section 1.5](https://arc4cfd.github.io/section1/part5/) of this course when solving the 2D Poisson equation, there are 2 common ways of running large-scale simulations on the cluster: (i) an interactive session by logging into the compute nodes directly, and (ii) submitting a batch job to SLURM. For a very small simulation such as the 2D Poisson there is not much difference (in terms of time) between the 2 methods, however when the simulation becomes more involved, disadvantages of the former and advantages of the latter become more evident.

### Running in interactive mode
Now that we copied the **source** code from our `/home` directory into `/scratch`, our goal is to run 3 simulations starting from a coarse mesh ($\approx 200000$ points) to a fine mesh (of about $800000$ grid points). As we shall see in a later section, there is a reason behind this choice, but for now let's follow the steps towards running the simulation:

<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="OpenFOAM steps">
    1. **Allocate required HPC resources**:
    ```bash
    salloc -n 64 --time=10:00:0 --mem-per-cpu=3g --account=account-name
    ```
    2. **Create the sub-case directory** `bfs_200k_DDES` within the main case directory:
    ```bash
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ cp -r * ./case/* ./bfs_200k_DDES
    ```
    3. **Generate mesh** from file using the `gmsh` utility:
    ```bash
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ gmsh mesh/bfs_200k.geo -3 -o mesh/bfs_200k.msh -format msh2
    ```
    4. **Convert mesh to OpenFOAM format** and **modify boundary file to reflect boundary conditions**:
    ```bash
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ gmshToFoam mesh/bfs_200k.msh -case /bfs_200k_DDES
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ cp /bfs_200k_DDES/constant/polyMesh/boundary /bfs_200k_DDES/constant/polyMesh/boundary.old
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i '/physical/d' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i "/wall_/,/startFace/{s/patch/wall/}" /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i "/top/,/startFace/{s/patch/symmetryPlane/}" /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i "/front/,/startFace/{s/patch/cyclic/}" /bfs_200k_DDES/constant/polyMesh boundary                                        
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i "/back/,/startFace/{s/patch/cyclic/}" /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/front/,/}/{/startFace .*/a'"\\\tneighbourPatch  back;" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/back/,/}/{/startFace .*/a'"\\\tneighbourPatch  front;" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/cyclic/,/nFaces/{/type .*/a'"\\\tinGroups        1(cyclic);" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@ggra796 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/wall_/,/}/{/type .*/a'"\\\tinGroups        1(wall);" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    ```

    5. **Start the simulation**:
    ```bash
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ cd ./bfs_200k_DDES
    [username@gra796 ~/scratch/01_BFS_openFOAM]$ ./Allrun
    ```
    Where the `Allrun` script performs some very important operations. Among them:
    ```bash
    cp -r 0.orig 0                # initialize the flow
    runApplication decomposePar   # decomposed the domain based on the # of procs.
    runParallel $(getApplication) # run the application in parallel
    runApplication reconstructPar # after simulation is done join processors into single file
    rm -rf processor*             # remove all the single processors directories
    ```
    After **step 5** is completed, you should see the simulation starting on the terminal:
    ```bash
    Running decomposePar on /home/ambrox/scratch/BFS_OpenFOAM/bfs_200k_DDES
    Running pimpleFoam in parallel on /home/ambrox/scratch/BFS_OpenFOAM/bfs_200k_DDES using 64 processes
    ```
    At this point the terminal window *hangs* while the simulation runs. If the terminal window is closed the simulation **stops**.
    </TabItem>
    <TabItem label="SU2 steps">
    1. **Allocate required HPC resources**:
    ```bash
    salloc -n 64 --time=10:00:0 --mem-per-cpu=3g --account=account-name
    ```
    2. **Generate mesh** from file using the `gmsh` utility:
    ```bash
    [username@gra796 ~/scratch/02_BFS_SU2/Coarse]$ gmsh Backstep_str_mesh.geo -0
    ```
    3. **Start the simulation**:
    ```bash
    [username@gra796 ~/scratch/02_BFS_SU2/Coarse]$ mpirun -n 64 SU2_CFD Backstep_str_config.cfg
    ```    
    </TabItem>
</Tabs>
</Box>

Here are some *PROs* and *CONs* of running a large scale simulation in interacive mode:
<CustomAside icon="star" title="PROs" colour="green">
1. Easier to set up.
2. Easier to run. 
</CustomAside>

<CustomAside icon="warning" title="CONs" colour="red">
1. **Not suited** for **LONG** jobs as the terminal window must stay open and the workstation on.
2. **Not suited** when running several simulations. 
</CustomAside>

### Submitting a batch script
In most of cases, a CFD engineer or PhD student would need to run a large series of numerical simulations, often with different geometries, boundary conditions, mesh sizes, etc. As you might have already guessed, running $N$ simulations in interactive mode not only becomes a tedious repetition of the same 5 steps (in the OpenFOAM case), but it also increases the chances of making mistakes along the way. 

A **very good** idea in HPC when dealing with repeatitive tasks is the concept of **automation**. In this case, for instance, we could include Steps 1-5 in a single file `run.sh` to be run in interacive mode, or **even better** in a batch job script `run_jobscript.sh` to submit to the job scheduler. Both files are included in the GitHub repository and are shown below:

<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="OpenFOAM steps">
    <details>
        <summary>run.sh</summary>
        <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/master/Section2/OpenFoam/run.sh' lang='bash' meta="title='run.sh' " />
    </details>
    <details>
        <summary>run_jobscript.sh</summary>
        <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/master/Section2/OpenFoam/run_jobscript.sh' lang='bash' meta="title='run_jobscript.sh' " />
    </details>
    </TabItem>
    <TabItem label="SU2 steps">
    <details>
    <summary>su2job_StdEnv.sh</summary>
        <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/master/Section2/SU2/Coarse/su2job_StdEnv.sh' lang='bash' meta="title='su2job_StdEnv.sh' " />
    </details>
    </TabItem>
</Tabs>
</Box>

The command to submit the batch script is simply:
<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="OpenFOAM steps">
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sbatch run_jobscript.sh
    Submitted batch job 26236582 
    ```
    </TabItem>
    <TabItem label="SU2 steps">
    ```bash
    [username@gra-login1 ~/scratch/02_BFS_SU2/Coarse]$ sbatch su2job_StdEnv.sh
    Submitted batch job 26236582 
    ``` 
    </TabItem>
</Tabs>
</Box>

<Box iconName='quiz'>
### Problem 1
Run the numerical simulation of the same backward facing step flow for the mesh containing $\approx 400000$ grid points. 

What would be the time step size $\Delta t$ required by the CFL stability condition?
<MultipleChoice>
    <Option>
      The same
    </Option>
    <Option  isCorrect>
      $4\times 10^{-5}$   
    </Option>
    <Option>
      $2\times 10^{-5}$   
    </Option>
</MultipleChoice>

What would be the writeInterval required to still print results every 2 milliseconds?
<MultipleChoice>
    <Option>
      20
    </Option>
    <Option  isCorrect>
      50   
    </Option>
    <Option>
      40   
    </Option>
</MultipleChoice>
</Box>


## Perform a runtime analysis of the simulation
Once the job is submitted, we should make sure the simulation is running properly. This is done by typing the command:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sq
JOBID     USER      ACCOUNT       NAME    ST  TIME_LEFT   NODES CPUS TRES_PER_N MIN_MEM NODELIST (REASON) 
26236582  username  def-piname  bfs_DDES   R   19:59:46     8    64        N/A    3G    gra[11005,11007,11010,11012-11016] (None)
```

The code is running, as expected, on 64 processors using 8 nodes. This check however, does not really tell us that **everything is going well** but only that the 64 processes have started and are working at something. The next step would be to check the **log file**.

<Box color='clear'>
<Tabs group="tab-group">
    <TabItem label="OpenFOAM">
         If you recall, with the command `mpirun pimpleFoam -parallel > log.pimpleFoam` in the **run_jobscript.sh** we asked the code to write any output to a log file called `log.pimpleFoam`. If you notice, this file popped up into our case directory (`bfs_200k_DDES`) as soon as the simulation started. 

        Depending on how far along in the simulation you are, the `log.pimplefoam` file might be quite long. To give you a quick run through of how it looks, let's visualize the beginning of it:

        <details>
            <summary>See log.pimpleFoam</summary>
            ```bash
            [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ vim log.pimpleFoam
            Starting time loop

            Courant Number mean: 0.37558691 max: 5.0866216
            Time = 0.0081

            PIMPLE: iteration 1
            DILUPBiCGStab:  Solving for Ux, Initial residual = 0.0543257, Final residual = 0.0030492453, No Iterations 1
            DILUPBiCGStab:  Solving for Uy, Initial residual = 0.019132138, Final residual = 0.0009267347, No Iterations 1
            DILUPBiCGStab:  Solving for Uz, Initial residual = 0.066712166, Final residual = 0.0049700607, No Iterations 1
            GAMG:  Solving for p, Initial residual = 0.98050352, Final residual = 0.021488268, No Iterations 2
            time step continuity errors : sum local = 0.00044866542, global = -1.3391353e-05, cumulative = -1.3391353e-05
            GAMG:  Solving for p, Initial residual = 0.14858043, Final residual = 8.6866034e-07, No Iterations 45
            time step continuity errors : sum local = 3.0582936e-08, global = -3.2758568e-09, cumulative = -1.3394628e-05
            PIMPLE: iteration 2
            DILUPBiCGStab:  Solving for Ux, Initial residual = 0.058760738, Final residual = 0.003117804, No Iterations 1
            DILUPBiCGStab:  Solving for Uy, Initial residual = 0.021669178, Final residual = 0.00078736235, No Iterations 1
            DILUPBiCGStab:  Solving for Uz, Initial residual = 0.12996742, Final residual = 0.010186982, No Iterations 1
            GAMG:  Solving for p, Initial residual = 0.59370611, Final residual = 0.014856221, No Iterations 2
            time step continuity errors : sum local = 0.00034748909, global = 1.1426898e-05, cumulative = -1.9677303e-06
            GAMG:  Solving for p, Initial residual = 0.30867957, Final residual = 9.1670175e-07, No Iterations 49
            time step continuity errors : sum local = 1.3500471e-08, global = 1.43852e-09, cumulative = -1.9662918e-06
            PIMPLE: iteration 3
            DILUPBiCGStab:  Solving for Ux, Initial residual = 0.015980338, Final residual = 0.0006615279, No Iterations 1
            DILUPBiCGStab:  Solving for Uy, Initial residual = 0.0087843757, Final residual = 0.00022240436, No Iterations 1
            DILUPBiCGStab:  Solving for Uz, Initial residual = 0.11693161, Final residual = 0.006951935, No Iterations 1
            GAMG:  Solving for p, Initial residual = 0.65043891, Final residual = 0.010075974, No Iterations 2
            time step continuity errors : sum local = 0.00010854745, global = 2.6004423e-06, cumulative = 6.3415051e-07
            GAMG:  Solving for p, Initial residual = 0.40140073, Final residual = 9.079401e-07, No Iterations 47
            time step continuity errors : sum local = 4.8308146e-09, global = 5.1590399e-10, cumulative = 6.3466642e-07
            ```
        </details>

        Important information to retain from the log file are:
        1. **The time iteration** corresponds to the time integration of the equations of motion mentioned in [section 2.3](https://arc4cfd.github.io/section2/part3/).
        2. **The CFL** or Courant number is displayed at every time stamp.
        3. **Residuals** are shown at each iteration for all velocity components and pressure.
        4. **Local and global** mass conservation is also printed at each iteration.

        These 4 pieces of information are already incredibly useful to understand if the simulation is converging, diverging, if mass is globally conserved, or if there is a problem in the domain. 

        The simulation will be running probably for several hours, and the ideal scenario is that every once in a while we check the behavior of the residuals and mass conservation. As you might guess staring at numbers on the screen is not the best approach and, once again, it is better to adopt an automated mechanism to visualize residuals. This can be done using **gnuplot**, a command-line and GUI program that can generate two- and three-dimensional plots of functions, data, and data fits. Gnuplot is usually present by default in any UNIX system, however to make sure you have it in your profile on the cluster, you can type:
        ```bash
        [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gnuplot

            G N U P L O T
            Version 5.4 patchlevel 2    last modified 2021-06-01 

            Copyright (C) 1986-1993, 1998, 2004, 2007-2021
            Thomas Williams, Colin Kelley and many others

            gnuplot home:     http://www.gnuplot.info
            faq, bugs, etc:   type "help FAQ"
            immediate help:   type "help"  (plot window: hit 'h')

        ```
        If you don't see the gnuplot welcome message, or if the terminal throws you an error, you can load the gnuplot module just like any other module:
        ```bash
        [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ module load gnuplot 
        [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ module save
        ```

        We can now write a simple script to plot residuals during runtime:
        <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/master/Section2/OpenFoam/plot_residuals.gnu' lang='bash' meta="title='plot_residuals.gnu' frame='code' mark={21}" />

        The script above will plot the residuals for all velocity components during the past 40 time steps. The students can modify the `highlighted line` in the script to change the plotting range. The script **MUST BE** located in the same directory of the `log.pimpleFoam`, and to run it simply type:

        ```bash
        [username@gra-login1 ~/scratch/01_BFS_openFOAM/bfs_200k_DDES]$ gnuplot plot-residuals
        ```

        ![Residuals.](../../../assets/figs_section2/residual-plot.png "Residuals.")
        <Caption>Residuals of the three velocity components, $U_x$, $U_y$, and $U_z$.</Caption>


    </TabItem>
    <TabItem label="SU2">
        If you recall, in lines 252 to 255 of `Backstep_str_config.cfg` file, we have instructed SU2 to generate an output file containing convergence history under the name, `history.csv`. If you notice, this file popped up into our case directory (`02_BFS_SU2/Coarse/`) as soon as the simulation started.

        Depending on how far along in the simulation you are, the `history.csv` file might be quite long. To give you a quick run through of how it looks, let's visualize the beginning of it:

        <details>
            <summary>See history.csv</summary>
            ```bash 
            1 "Time_Iter","Outer_Iter","Inner_Iter",     "rms[P]"     ,     "rms[U]"     ,     "rms[V]"     ,     "rms[W]"     ,     "rms[nu]"               
            2           0,           0,           4,       -8.36488691,      -8.323185486,      -8.749827447,      -6.595921079,      -13.98444469
            3           1,           0,           4,       -8.12803185,      -8.128927621,      -8.208809081,      -6.192896793,      -14.06565528
            4           2,           0,           4,      -7.960896066,      -7.959261575,      -7.869191588,      -5.866127477,      -14.20445358
            5           3,           0,           4,       -7.87770485,      -7.865723278,      -7.730622795,      -5.674950425,      -14.33212574
            6           4,           0,           4,      -7.851101629,      -7.795018166,      -7.707723687,      -5.578550417,      -14.39499683
            7           5,           0,           4,      -7.848972586,      -7.697854479,      -7.734610289,      -5.498682789,      -14.51260334
            8           6,           0,           4,      -7.839963907,      -7.596020898,      -7.758581592,      -5.383722204,      -14.46793238
            9           7,           0,           4,       -7.84740461,       -7.54672322,       -7.79919976,        -5.3481669,      -14.45351153
            10          8,           0,           4,      -7.903200003,      -7.571866226,       -7.89494942,      -5.392784656,      -14.54477387
            ```
        </details>

        In this case, the file contains the root-mean-square $rms$ for all variables ($u$,$v$,$w$,$p$, and $\nu$), **however**, in SU2 the output from the code can be customized quite extensively based on the usere interest, but here is a general terminology:

        1. **Screen output**: The convergence history printed on the console.

        2. **History output**: The convergence history written to a file.

        3. **Volume output**: Everything written to the visualization and restart files.
        
        4. **Output field**: A single scalar value for screen and history output or a vector of a scalar quantity at every node in the mesh for the volume output.
        
        5. **Output group**: A collection of output fields.

        More information can be found [HERE](https://su2code.github.io/docs_v7/Custom-Output/). The simulation will be running probably for several hours, and the ideal scenario is that every once in a while we check the behavior of the $rms$. As you might guess staring at numbers on the screen is not the best approach and, once again, it is better to adopt an automated mechanism to visualize $rms$. This can be done using **gnuplot**, a command-line and GUI program that can generate two- and three-dimensional plots of functions, data, and data fits. Gnuplot is usually present by default in any UNIX system, however to make sure you have it in your profile on the cluster, you can type:

        ```bash
        [username@gra-login1 ~/scratch/02_BFS_SU2/Coarse]$ gnuplot

            G N U P L O T
            Version 5.4 patchlevel 2    last modified 2021-06-01 

            Copyright (C) 1986-1993, 1998, 2004, 2007-2021
            Thomas Williams, Colin Kelley and many others

            gnuplot home:     http://www.gnuplot.info
            faq, bugs, etc:   type "help FAQ"
            immediate help:   type "help"  (plot window: hit 'h')

        ```
        If you don't see the gnuplot welcome message, or if the terminal throws you an error, you can load the gnuplot module just like any other module:

        ```bash
        [username@gra-login1 ~/scratch/02_BFS_SU2/Coarse]$ module load gnuplot 
        [username@gra-login1 ~/scratch/02_BFS_SU2/Coarse]$ module save
        ```

        We can now write a simple script to plot $rms$ values during runtime:

        <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/master/Section2/SU2/Coarse/plot_residuals.gnu' lang='bash' meta="title='plot_residuals.gnu' frame='code' mark={10-21}" />

    The students can modify the **highlighted lines** in the script above to change the plotting range. The script **MUST BE** located in the same directory of the `lhistory.csv`, and to run it simply type:

    ```bash
    [username@gra-login1 ~/scratch/02_BFS_SU2/Coarse]$ gnuplot plot-residuals
    ```

    ![Residuals.](../../../assets/figs_section2/residual-plot-su2.png "Residuals.")
    <Caption>Standard deviation $rms$ of the three velocity components, $U_x$, $U_y$, and $U_z$.</Caption>

    </TabItem>
</Tabs>
</Box>

## You might want to think about output files
When running a simulation in parallel it is **CRUCIAL** to think about the impact of the output files on the HPC workflow. Running $N$ simulations without thinking about **I/O** penalty will cause you A LOT of trouble both in terms of bookkeeping and in terms of disk quota.

### Number of output files
Depending on the CFD tool used, when running a simulation in parallel we need to remember that the computational domain has been decomposed into $N$ processes. Therefore, if one was to save a snapshot every time step and integrate the equations of motion for $N_{t}$ time steps, **every single processor** will output data at **each** timestep. Assuming that the code is outputting 5 variables ($u$, $v$, $w$, $P$, and $\nu_{t}$), this will give rise to $N\times N_{t}\times 5$ files in our `/scratch` directory. 

:::danger
In simple terms, the coarse simulation of the BFS we have just carried out on 64 processors for about 25000 iterations would generate about 8 million files!! This is why the **time interval** between snapshots should be chosen wisely.
:::

This type of output is known as **parallel output**, and one should always consider **merging** all processors' files at each time stamp after the simulation is done or (if possible) during runtime. This is exacly why `reconstructPar` was included in the OpenFOAM batch script file. 

### Example: estimating number of files
<Box iconName='exercise'>
Even though some CFD tools available will perform this operation by default, it is always a good idea to perform a rough estimate of the number of output files expected from a numerical simulation. Let's consider our BFS simulation over a total time of $T=0.5\,s$, saving snapshots every $2\, ms$, on 64 processors.

1. **Number of files parallel output**:

$$N_{files} = 64\times 250\times 5 = 80000$$

2. **Number of files merged output**:

$$N_{files} = 250\times 5 = 1250$$

</Box>

### Size of output files
Some thought should also be given to the size of the output files. Without going int0 too much detail, in HPC we have two possible output formats:

1. **Binary**: as mentioned in [section 1](https://arc4cfd.github.io/section1/outline/) of this course, the binary language is very efficient for programs and is not designed for humans to read. **Executables** for instance are written in binary code by the compiler, and contain the set of instructions a program has to execute.

2. **ASCII**: stands for American Standard Code for Information Interchange. It is a coded character set consisting of 128 7-bit characters. There are 32 control characters, 94 graphic characters, the space character, and the delete character. ASCII is a way of writing files that can be easily read by humans. A very common text file (.txt) is an ASCII file.

**Why does this matter in CFD and HPC?**

:::note[In general]
Binary style is faster for **read/write** since the machine does not have to convert to a human-readable format. The size of a binary output file is also smaller as compared to an ASCII file.
:::

Applying this reasoning to a CFD case:

1. For complex geometries and very large mesh files where the goal is to print the output for hundreds or thousands of snapshots, **binary** would be a better choice, as writing many data points and many snapshots can be done relatively instantaneously (compared to converting and writing thousands of ASCII files).

2. For relatively small cases on simple geometries, where the number of output files is not too large, writing ASCII files will not cause a significant performace hit, and one can open and manipulate single output files.

To Summarize:
<CustomAside icon="star" title="PROs" colour="green">
1. **ASCII**:
    - Suitable for small meshes and few snapshots. 
    - Can be visualized and edited using regular text editors.
2. **Binary**:
    - Smaller file size.
    - Faster to read/write
    - Suitable for large meshes and complex geometries.
</CustomAside>

<CustomAside icon="warning" title="CONs" colour="red">
1. **ASCII**:
    - Larger file size. 
2. **Binary**:
    - Cannot be read and edited by regular text editors.
</CustomAside>

:::tip[Tip]
Check the documentation of your CFD tool as you might be able to change how output files are written. In OpenFOAM, for instance, you can switch between the two write methods on the fly by modifying the `controlDict` entry while your case is running.
:::



### Let's visualize the flow!
(more on this next class)
<YouTube id='aw0ccZK-erg' />

:::note[Learning Objectives]
Having finished this lecture, you should now be able to answer the following important questions:
1. How do I organize simulation files on the cluster?
2. How do I run a large-scale CFD simulation on HPC systems?
3. How do I monitor the simulation during runtime?
4. How do I save data efficiently?
:::


