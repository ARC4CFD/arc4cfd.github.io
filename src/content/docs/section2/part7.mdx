---
title: 'A posteriori analysis and visualization'
---

import Box from '../../../components/Box.astro';
import Caption from '../../../components/Caption.astro';
import Option from '../../../components/Option.astro';
import CustomAside from '../../../components/CustomAside.astro';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import CodeFetch from '../../../components/CodeFetch.astro';
import MultipleChoice from '../../../components/MultipleChoice.astro';


:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Organize and plan result files on the cluster
2. Perform common 'a posteriori' analysis technique in CFD
3. Visualize flow field using various methods
:::

## Postprocessing CFD simulations on HPC: Overview
![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_workflow_POSTPROC.png "Process simulations of the CFD simulation")

After completing [section 2.6](https://arc4cfd.github.io/section2/part6/), our large-scale CFD simulation is now running on the HPC system, and printing results as specified by the user. We also learned how to monitor the simulation using simple line commands on terminal, and the very useful plotting tool [Gnuplot](http://www.gnuplot.info/). At this point **the fun part** begins as we can dive into the visualization of the results. Before THAT however, **be mindful and ...**


## Organize and plan result files on the cluster
Assuming that your simulation ran for **enough** time based on your estimate, the job is now **completed** and results can be moved in the **long term** storage where analysis can be performed on them without worrying about data getting purged by the system administrators. The **long term** storage unit, as mentioned in [section 2.6](http://localhost:4321/section2/part6), is the `/projects` directory connected to your supervisor's (or principal investigator) account. Although, the `/project` space will probably have hundreds of terabytes (TB) of storage capability, IT WILL ALWAYS have a limit, especially in the **total number** of files that can be stored. 

:::note[Keep in mind]
1. `/projects` is a **group** directory, meaning that whatever you do inside it **could affect** and **potentially harm** the workflow of other researchers in the same group.
2. **Always merge parallel output files before moving them** to `/project`. This is a crucial step as we saw how a very simple simulation like the BFS on 64 processors generated close to 100 thousand files. 
3. The name of the directory (`/projects`) is specific to Compute Ontario HPC systems, however, the concept of **long-term data storage** and the reccommended process we outlined, apply to any HPC system.
:::

So far we have seen 2 common ways to move files between two locations in the HPC system: the `cp` command, and the `sftp` protocol. We will show below three more **secure** and **much faster** ways of transferring LARGE quantities of data.

:::caution[Data transfer nodes]
When moving large amount of data, the user **should use** the dedicated data transfer nodes, also called **data mover nodes**. Just like login nodes these are dedicated nodes for the sole purpose of data transfer. If available, you will find the URL on the documentation of each respective HPC system. For instance, Graham has dedicated **data transfer nodes** under the URL `gra-dtn1.alliancecan.ca`. To login into the data transfer nodes simply type:
```bash
[username@gra-login1 ~]$ ssh username@gra-dtn1.alliancecan.ca
```
:::

### The Secure Copy Protocol (SCP)
The secure copy protocol or simply `scp` is a command-line tool to **securely** transfer data. The **very important** feature of `scp` is that all files and passwords that are being transferred are encrypted, so that any third-party user analyzing the traffic of data, cannot retreive any sensitive information. `scp` is therefore highly recommended when dealing with sensitive or proprietary data. The basic syntax for `scp` is:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ scp options user@source_host:/path-to-files user@dest_host:/path-to-dir
```
Where:
- **Options**: the user might specify options to change the behavior of `scp`. Commonly used options are:
    1. `-P`: specifies the **port** to connect on the remote host.
    2. `-p`: preserves **modification times, access times, and modes** from original files.
    3. `-r`: recursively copy entire directories.

- **user@source_host:/path-to-files**: this is the path of the local or remote file(s) we are about to transfer. 
- **user@dest_host:/path-to-dir**: this it the path to the final destination of the files(s) we are about to transfer.

### Remote sync 
The remote sync os simply `rsync` is a command-line, remote and local file synchronization tool. `rsync` is very fast and the most important feature is that it uses and algorithm that minimizes the amount of data **copied** by only moving the portion of files that are different between the **source** directory and the **destination** directory. Therefore, `rsync` is extremely useful for a periodic backup of data on the HPC system. The basic syntax for `rsync` is:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ rsync options user@source_host:/path-to-files user@dest_host:/path-to-dir
```
Where the structure and syntax of the command is very similar to `scp`. To see what options are available for `rsync` click [here](https://linux.die.net/man/1/rsync).

### Globus transfer
The easiest, fastest, and most reliable way of transferring TB and even PB of data between any two locations is a **Globus transfer**.

[Globus](https://docs.alliancecan.ca/wiki/Globus) is a service for fast, reliable, secure transfer of files. Designed specifically for researchers, Globus has an easy-to-use interface with background monitoring features that automate the management of file transfers between any two resources, whether they are at on Alliance cluster, another supercomputing facility, a campus cluster, lab server, desktop or laptop.

Globus leverages GridFTP for its transfer protocol but shields the end user from complex and time-consuming tasks related to GridFTP and other aspects of data movement. It improves transfer performance over GridFTP, rsync, scp, and sftp, by automatically tuning transfer settings, restarting interrupted transfers, and checking file integrity.

Upon loggin in the Globus portal using your Compute Canada credentials (same `username` and `password` used to connect to the remote cluster) the Globus welcome page will look something like this:

![Globus.](../../../assets/figs_section2/globus-welcome.png "Globus.")
<Caption>Globus welcome page.</Caption>

Globus has a very user-friendly interface, and the user can split the page to have **source files** on the left, and **destination directory** on the right. At this point the only step left is to find the Globus **end-point** for the specific cluster. Since we are working on Graham, its globus end point is `computecanada#graham-globus`. If you are working with a different Compute Ontario HPC system, please visit the respective documentation page ([Graham](https://docs.alliancecan.ca/wiki/Graham), [Niagara](https://docs.alliancecan.ca/wiki/Niagara), [Narval](https://docs.alliancecan.ca/wiki/Narval), [Beluga](https://docs.alliancecan.ca/wiki/B%C3%A9luga), and [Cedar](https://docs.alliancecan.ca/wiki/Cedar)).

By pasting the **globus endpoint** on the top-left seach bar the user will be taken to a visual of the `home` directory, and can directly navigate to the directory containing all the files that must be moved. Assuming that files must be transferred on the same cluster, the user follow the same process on the right side navigating in the directory where data must be transferred to. To start the transfer simply push the button `start` on the top left side. This will initiate the transfer left-right of the selected directories. The user can also monitor the activity by navigating under the `activity` tab of the left-bar options. When the transfer is completed, the user will receive a confirmation email. 

## Visualization at LAST!
At last we come to the fun part of **ARC4CFD** which is: data visualization. If you are a first-time HPC user, you probably have done most of your visualization on your local machine (desktop or laptop) using your preferred tool. The first thing you'd probably want to do is **download** your freshly computed data on your local machine and embark in a **nice** visualization session. HOWEVER, you will quickly realize that the data you generated can be quite **massive**. A time-dependent, large-scale, 3D, CFD simulation will generate thousands of snapshots that can be anywhere from 2-3GB to tens of GB. The size of a complete simulation will therefore be in the order of several TB. **Downloading** this much data in your local machine, even assuming you have the space for it, it's probably not the best use of your computational resources.

Moreover, even if you DO manage to download *some* data on your local machine, most machines have up to 32GB of RAM (most commonly 8-16GB) and you will probably reach **memory limit** when trying to open and analyze the data using your preferred tool.

:::danger[Problems]
1. The dataset is **WAY TOO LARGE** to be downloaded locally.
2. Individual snapshots **WILL NOT** fit into your computer's memory.
3. Rendering and analyzing the dataset will be **TOO SLOW** on your computer.
:::

<CustomAside icon="star" title="Solution" colour="green">
Leave the datasets on the HPC system and analyze the data remotely. **Don't worry**, you won't have to give up to your preferred visualization software; most likely you will only need to learn a new way to use it. **Keep in mind that**:
1. Compute nodes have **WAY MORE** memory than your local machine will ever have.
2. The **dataset can be analyzed in parallel** by spreading the memory accross multiple processors (just like we ran the simulation in parallel).
3. The **rendering will be mush faster** with the added benefit of using GPUs on the cluster.
</CustomAside>

In this section the goal is to present a wide variety of opportunities offered by the HPC system in terms of visualization. This is not, BY ANY MEANS, a complete and conclusive list, and the students are also given external resources to learn more. Here we will present three common types of visualization on the remote HPC system:
1. **Batch visualization**
2. **Interactive remote desktop visualization**
3. **Interactive client-server visualization**

### Batch visualization
This is probably the easiest and most effective way of visualizing your data on the cluster. If you have used Matplotlib before, or other python libraries for visualiztion, you probably know that this tools (`plt.show()`) open a separate window with the figure we have generated. By now you might think that this is the **standard** approach of data visualization, but **you would be wrong**.

:::note[Batch = NO screen interacion]
Batch visualization has no GUI interaction. The step we follow are:
1. Write your visualization algorithm in a precompiled script.
2. Use the command line to run the script **interactively** or as a SLURM job (reccommended for heavy rendering jobs).
:::

<CustomAside icon="star" title="PROs of Batch visualization" colour="green">
1. Very easy to automate tasks.
2. Well organized and documented workflow, which improves reproducibility.
3. Allows visualization without any GUI elements.
4. Many open-source tools have scripting interfaces:
    - **Python libraries** (e.g. [Matplotlib](https://matplotlib.org/stable/))
    - Some **domain-specific packages** support scripting (e.g. Visual Molecular Dynamics ([VMD](https://www.ks.uiuc.edu/Research/vmd/)) provides python interface)
    - Common **general purpose tools** provide scripting interfaces (e.g. [ParaView](https://www.paraview.org/), [VTK](https://vtk.org/), [Blender](https://www.blender.org/))
</CustomAside>


:::caution[Disclaimer]
Below we will use **Python** and **ParaView** to carry out examples, but the same process will apply to any other programming language or general purpose tools.
:::

The first step to use Python as a batch visualization tool is to install all the required Python packages into a **virtual environment** on the cluster that can be loaded at our convenience. Upon loggin into Graham or any other HPC system type:
```bash title='Create virtual environment'
[username@gra-login1 ~]$ module load python/3.10.13 mpi4py
[username@gra-login1 ~]$ virtualenv --no-download ~/theshire    # install python pckgs into your ~/theshire
[username@gra-login1 ~]$ source ~/theshire/bin/activate
(theshire) [username@gra-login1 ~]$ pip install --no-index --upgrade pip
(theshire) [username@gra-login1 ~]$ pip install --no-index matplotlib numpy plotly h5py
(theshire) [username@gra-login1 ~]$ pip install yt
(theshire) [username@gra-login1 ~]$ deactivate
[username@gra-login1 ~]$
```
Once the environment has been created, it can be `sourced` anytime and from any location on the cluster. The reccommended post processing workflow would be:
```bash title='Virtual environment workflow'
[username@gra-login1 ~]$ source ~/theshire/bin/activate
(theshire) [username@gra-login1 ~]$ python my-post-pro-routine.py
(theshire) [username@gra-login1 ~]$ python my-other-post-pro-routine.py
(theshire) [username@gra-login1 ~]$ deactivate
[username@gra-login1 ~]$
```

Let's take one of the many [examples](https://matplotlib.org/stable/gallery/index.html) available on the Matplotlib web page, and let's use it to generate a PNG figure on the cluster. 

```python title='tri-colour.py' {5} {30-34}
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.tri as tri
mpl.use('Agg')                 # This enables PNG backend
 
# First create the x and y coordinates of the points.
n_angles = 48
n_radii = 8
min_radius = 0.25
radii = np.linspace(min_radius, 0.95, n_radii)
 
angles = np.linspace(0, 2 * np.pi, n_angles, endpoint=False)
angles = np.repeat(angles[..., np.newaxis], n_radii, axis=1)
angles[:, 1::2] += np.pi / n_angles
 
x = (radii * np.cos(angles)).flatten()
y = (radii * np.sin(angles)).flatten()
z = (np.cos(radii) * np.cos(3 * angles)).flatten()
 
# Create the Triangulation; no triangles so Delaunay triangulation created.
triang = tri.Triangulation(x, y)
 
# Mask off unwanted triangles.
triang.set_mask(np.hypot(x[triang.triangles].mean(axis=1),
                          y[triang.triangles].mean(axis=1))
                 < min_radius)
 
# Plotting
plt.figure(figsize=(8,8))
plt.tripcolor(triang,z,shading='flat')
plt.title('Tripcolor test',fontdict={'fontsize' : 15})
plt.savefig('tripcolor.png')
```

Where the only modifications we have made are the highlighted lines in the code to (i) allow PNG backend, and (ii) generate and save the figure as PNG file. The script can now be run in sevrral ways (as seen in [section 1](https://arc4cfd.github.io/section1/outline/) of this course):
1. Directly on the login node (not advised).
2. As an interactive job on the compute node(s).
3. As a bacth job submitted to SLURM.

![Tripcolor plot.](../../../assets/figs_section2/tripcolor.png "Tripcolor.")
        <Caption>Output of the tri-colour.py script.</Caption>

### Interactive remote desktop visualization
Sometimes it's necessary to work with a GUI, whether we are doing data analysis using Matlab or some heavy rendering using Paraview. The first great option that HPC systems allow to remotely visualize data on the cluster is the use of a **Virtual Network Computing** connection. In very simple terms VNC is a cross-platform screen sharing system that was developed to remotely control another computer.

:::caution[Required step]
The students will need to install a **VNC client** on the local machine to connect to the VNC remote server. Compute Ontario reccommend the use of [TigerVNC](https://tigervnc.org/) as it's a widely use VNC client, and it's available on Windows, MacOS, and most Linux distributions. For detailed steps on how to install TigerVNC on your local machine, click [**HERE**](https://docs.alliancecan.ca/wiki/VNC).
:::

As you might have guessed, since this is a client/server connection, now that we have set up the **client** on our local machine, we need the VNC server to connect to. Here there are 2 options based on the HPC system you are connected to:
1. **Graham**: Graham has dedicated **VDI nodes** collectively known as gra-vdi. These nodes provide a full graphical desktop, accelerated OpenGL, and shared access to Graham's `/home`, `/project`, and `/scratch` filesystems.
2. **ALL CO HPC systems**: the user must initiate a temporary VNC server. This can be done on both **login** or **compute nodes**. The process is quite involved in terms of steps to follow, and we will not focus too much on it. More detailed information can be found [**HERE**](https://docs.alliancecan.ca/wiki/VNC).

Upon starting a VNC desktop on your local machine (e.g. using TigerVNC) a small window pops up asking for the VNC server address. The user can paste `gra-vdi.computecanada.ca:5900` and click on `Connect`. You will be prompted on a login screen where you need to input your CO credentials (same credentials used to connect via SSH or SFTP). Nontice that on the top-left of the screen you read `gra-vdi3`, meaning that we are now connected to one of the VDI nodes on Graham. Upon connecting, the remote session will start ...

![VDI welcome.](../../../assets/figs_section2/vdi-welcome.png "VDI welcome.")
        <Caption>Welcome page when connecting to the VDI nodes in Graham.</Caption>

To open Matlab or Paraview, the user can open a terminal windown in the VNC remote session and type:
<Tabs group="tab-group">
    <TabItem label="Paraview">
        ```bash
        [username@gra-vdi3]~: module load SnEnv
        [username@gra-vdi3]~: module load paraview/5.9.1
        [username@gra-vdi3]~: paraview
        ```
    </TabItem>
    <TabItem label="Matlab">
        ```bash
        [username@gra-vdi3]~: module load CcEnv 
        [username@gra-vdi3]~: module load StdEnv/2018 
        [username@gra-vdi3]~: module load matlab
        [username@gra-vdi3]~: matlab
        ```
    </TabItem>
</Tabs>

After which the preferred software will pop up on the screen, and the user can start the visualization session as usually done on the local machine.
![VDI paraview.](../../../assets/figs_section2/vdi-paraview.png "VDI paraview.")
        <Caption>Paraview opened on screen using remote VNC on Graham.</Caption>

:::danger[Caution]
If you want to perform computational tasks within a GUI, please do so on a cluster compute node using the `salloc` command as described in the Compute Nodes section [**HERE**](https://docs.alliancecan.ca/wiki/VNC). This will ensure the memory and CPU resources on gra-vdi are fully available for interactive graphical visualization purposes by other users when needed.
:::

:::note[Beware]
When using interactive **remote-desktop** visualization, both the datasets and the rendering ARE ON the cluster. You will probably have solved memory issues since the VDI nodes have a much larger memory compared to your local machine, however you might experience some LAG during rendering as the **screenshots** from the VNC Server are trasmitted **across the network** to the VNC Viewer. 
:::

This small issue can be solved by ...

### Interactive client-server visualization
In the client-server visualization method, all the graphical tools are running locally (on your laptop or desktop) while the dataset is on the remothe machine. In other words, taking the Praview example: the Paraview **client** will run on the local machine while the Paraview **server** will run on the cluster.

:::note[Important notes]
1. An important setting in ParaView's preferences is `Render View` -> `Remote/Parallel Rendering Options` -> `Remote Render Threshold`. If you set it to default (20MB) or similar, small rendering will be done on your computer's GPU, the rotation with a mouse will be fast, but anything modestly intensive (under 20MB) will be shipped to your computer and (depending on your connection) visualization might be slow. If you set it to 0MB, all rendering will be remote including rotation, so you will really be using the cluster resources for everything, which is good for large data processing but not so good for interactivity. **Experiment with the threshold to find a suitable value**.
2. **ParaView requires the same major version on the local client and the remote host**; this prevents incompatibility that typically shows as a failed handshake when establishing the client-server connection. For example, to use ParaView server version 5.10.0 on the cluster, you need client version 5.10.x on your computer.
:::

On Graham, Cedar, Beluga, and Narval you can do client-server rendering on both CPUs (in software) and GPUs (hardware acceleration). As suggested by the system administrators, it is always a good idea to start with CPU-only visualization asking as many cores as necessary.

:::tip[How many cores should we ask for?]
An easy way to estimate the number of cores required for rendering is to look at the size of the deataset loaded into memory at once (e.g. one snapshot), and divide it by $\approx 3.5 GB/core$. 
:::

In our coarse BFS simulation we ran in the previous section, we can get the size of each snapshot by running the command `du -sh timestep/` and you will get that it's about 30MB. Therefore allocating 3GB per cpu, for this case, is more than enough to allow a good rendering performace. In case the paraview process gets killed, we just have to increase the allocated memory.

The client-server connection is enstablished following 6 easy steps (as outlined [**HERE**](https://docs.alliancecan.ca/wiki/ParaView)):

1. **Start an interactive job** in the cluster where the data are stored:
    ```bash
    [username@gra-login1 ~]$ salloc --time=1:00:0 --ntasks=1 --mem-per-cpu=3g --account=def-piname
    ```
2. **Load the offscreen paraview** module and start the server:
    ```bash
    [username@gra796 ~]$ module load gcc/9.3.0 paraview-offscreen/5.11.0
    ```
    **IMPORTANT**: remember to load the same Paraview version you have installed locally.

3. **Start the server**:
    ```bash
    [username@gra796 ~]$ pvserver --force-offscreen-rendering
    Waiting for client...
    Connection URL: cs://gra796:11111
    Accepting connection(s): gra796:11111
    ```
    Take notes of the address `gra796` and port number `11111`.

4. **Link the port 11111 on your computer** and the same port on the compute node (make sure to use the correct compute node). Open another terminal windown on your local machine and type:
    ```bash
    [username@laptop ~]$ ssh username@graham.computecanada.ca -L 11111:gra796:11111
    ```
5. **Open Paraview and connect**: open Paraview on your local machine and navigate to `File` > `connect` > `Add server`. You will need to point ParaView to your local port 11111, so you can do something like `name = Graham`, `server type = Client/Server`, `host = localhost`, `port = 11111`; click Configure, select Manual and click Save. Once the remote is added to the configuration, simply select the server from the list and click on `Connect`. At this point the Graham terminal window will read:
    ```bash
    [username@gra796]~: pvserver --force-offscreen-rendering
    Waiting for client...
    Connection URL: cs://gra796:11111
    Accepting connection(s): gra796:11111
    Client connected.
    ```
6. **Open the desired file**: click on `File` > `Open`, and you will see that the Graham filesystem is now visible from the **local** Paraview client.


## Common 'a posteriori' analysis techniques in CFD [TO DO]
Explain the most important type of 'a posteriori' analysis [TO DO]. 
1. Instantaneous flow.
2. Long time average
3. Phase average

## Grid convergence and code validation
Before diving too much into endless visualization sessions of our fresh CFD data, we should **always** answer the question: **can I trust my results**?





Explain the importance of a grid sensitivity study and how to carry it out [TO DO]