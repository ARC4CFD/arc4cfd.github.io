---
title: 'A posteriori analysis and visualization'
---

import Box from '../../../components/Box.astro';
import Caption from '../../../components/Caption.astro';
import Option from '../../../components/Option.astro';
import CustomAside from '../../../components/CustomAside.astro';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import CodeFetch from '../../../components/CodeFetch.astro';
import MultipleChoice from '../../../components/MultipleChoice.astro';


:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Organize and plan result files on the cluster
2. Perform common 'a posteriori' analysis technique in CFD
3. Visualize flow field using various methods
:::

## Postprocessing CFD simulations on HPC: Overview
![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_workflow_POSTPROC.png "Process simulations of the CFD simulation")

After completing [section 2.6](https://arc4cfd.github.io/section2/part6/), our large-scale CFD simulation is now running on the HPC system, and printing results as specified by the user. We also learned how to monitor the simulation using simple line commands on terminal, and the very useful plotting tool [Gnuplot](http://www.gnuplot.info/). At this point **the fun part** begins as we can dive into the visualization of the results. Before THAT however, **be mindful and ...**


## Organize and plan result files on the cluster
Assuming that your simulation ran for **enough** time based on your estimate, the job is now **completed** and results can be moved in the **long term** storage where analysis can be performed on them without worrying about data getting purged by the system administrators. The **long term** storage unit, as mentioned in [section 2.6](http://localhost:4321/section2/part6), is the `/projects` directory connected to your supervisor's (or principal investigator) account. Although, the `/project` space will probably have hundreds of terabytes (TB) of storage capability, IT WILL ALWAYS have a limit, especially in the **total number** of files that can be stored. 

:::note[Keep in mind]
1. `/projects` is a **group** directory, meaning that whatever you do inside it **could affect** and **potentially harm** the workflow of other researchers in the same group.
2. **Always merge parallel output files before moving them** to `/project`. This is a crucial step as we saw how a very simple simulation like the BFS on 64 processors generated close to 100 thousands files. 
3. The name of the directory (`/projects`) is specific to Compute Ontario HPC systems, however, the concept of **long-term data storage** and the reccommended process we outlined apply to any HPC system.
:::

So far we have seen 2 common ways to move files between two locations in the HPC system: the `cp` command, and the `sftp` protocol. We will show below three more **secure** and **much faster** ways of transferring LARGE quantities of data.

:::caution[Data transfer nodes]
When moving large amount of data, the user **should use** the dedicated data transfer nodes, also called **data mover nodes**. Just like login nodes these are dedicated nodes for the sole purpose of data transfer. If available, you will find the URL on the documentation of each respective HPC system. For instanceof, Graham has dedicated **data transfer nodes** under the URL `gra-dtn1.alliancecan.ca`. To login into the data transfer nodes simply type:
```bash
[username@gra-login1 ~]$ ssh username@gra-dtn1.alliancecan.ca
```
:::

### The Secure Copy Protocol (SCP)
The secure copy protocol or simply `scp` is comman-line tool to **securely** transfer data. The **very important** feature of `scp` is that all files and passwords that are being transferred are encrypted, so that any third-party user analyzing the traffic of data, cannot retreive any sensitive information. `scp` is therefore highly recommended when dealing with sensitive or proprietary data. The basic syntax for `scp` is:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ scp options user@source_host:/path-to-files user@dest_host:/path-to-dir
```
Where:
- **Options**: the user might specify options to change the behavior of `scp`. Commonly used options are:
    1. `-P`: specifies the **port** to connect on the remote host.
    2. `-p`: preserves **modification times, access times, and modes** from original files.
    3. `-r`: recursively copy entire directories.

- **user@source_host:/path-to-files**: this is the path of the local or remote file(s) we are about to transfer. 
- **user@dest_host:/path-to-dir**: this it the path to the final destination of the files(s) we are about to transfer.

### Remote sync 
The remote sync os simply `rsync` is a command-line, remore and local file synchronization tool. `rsync` is very fast and the most important feature is that it uses and algorithm that minimizes the amount of data **copied** by only moving the portion of files that are different between the **source** directory and the *destination** directory. Therefore, `rsync` is extremely useful for a periodic backup of data on the HPC system. The basic syntax for `rsync` is:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ rsync options user@source_host:/path-to-files user@dest_host:/path-to-dir
```
Where the structure and syntax of the command is very similar to `scp`. To see what options are available for `rsync` click [here](https://linux.die.net/man/1/rsync).

### Globus transfer
The easiest, fastest, and most reliable way of transferring TB and even PB of data between any two locations is a **Globus transfer**.

[Globus](https://docs.alliancecan.ca/wiki/Globus) is a service for fast, reliable, secure transfer of files. Designed specifically for researchers, Globus has an easy-to-use interface with background monitoring features that automate the management of file transfers between any two resources, whether they are at on Alliance cluster, another supercomputing facility, a campus cluster, lab server, desktop or laptop.

Globus leverages GridFTP for its transfer protocol but shields the end user from complex and time-consuming tasks related to GridFTP and other aspects of data movement. It improves transfer performance over GridFTP, rsync, scp, and sftp, by automatically tuning transfer settings, restarting interrupted transfers, and checking file integrity.

Upon loggin in the Globus portal using your Compute Canada credentials (same `username` and `password` used to connect to the remote cluster) the Globus welcome page will look something like this:

![Globus.](../../../assets/figs_section2/globus-welcome.png "Globus.")
<Caption>Globus welcome page.</Caption>

Globus has a very user-friendly interface, and the user can split the page to have **source files** on the left, and **destination directory** on the right. At this point the only step left is to find the Globus **end-point** for the specific cluster. Since we are working on Graham, it's globus end point is `computecanada#graham-globus`. If you are working with a different Compute Ontario HPC system, please visit the respective documentation page ([Graham](https://docs.alliancecan.ca/wiki/Graham), [Niagara](https://docs.alliancecan.ca/wiki/Niagara), [Narval](https://docs.alliancecan.ca/wiki/Narval), [Beluga](https://docs.alliancecan.ca/wiki/B%C3%A9luga), and [Cedar](https://docs.alliancecan.ca/wiki/Cedar)).

By pasting the **globus endpoint** on the top-left seach bar the user will be take to a visual of the `home` directory, and can directly manually navigate to the directory containing all the files that must be moved. Assuming that files must be transferred on the same cluster, the user follow the same process on the right side navigating in the directory where data must be transferred to. To start the transfer simply push the button `start` on the top left side. This will initiate the transfer left-right of the selected directories. The user can also monitor the activity by navigating under the `activity` tab of the left-bar options. When the transfer is completed, the user will receive a confirmation email. 

## Visualization at LAST!
At last we come to the fun part of **ARC4CFD** which is: data visualization. If you are a first-time HPC user, you probably have done most of your visualization on your local machine (desktop or laptop) using your preferred tool. The first thing you'd probably want to do is **download** your freshly computed data on your local machine and embark in a **nice** visualization session. HOWEVER, you will quickly realize that the data you generated can be quite **massive**. A time-dependent, large-scale, 3D, CFD simulation will generate thousands of snapshots that can be anywhere from 2-3GB to tens of GB. The size of a complete simulation will therefore be in the order of several TB. **Downloading** this much data in your local machine, even assuming you have the space for it, it's probably not the best use of your computational resources.

Moreover, even if you DO manage to download *some* data on your local machine, most machines have up to 32GB of RAM (most commonly 8-16GB) and you will probably reach **memory limit** when trying to open and analyze the data using your preferred tool.

:::danger[Problems]
1. The dataset is **WAY TOO LARGE** to be downloaded locally.
2. Individual snapshots **WILL NOT** fit into your computer's memory.
3. Rendering and analyzing the dataset will be **TOO SLOW** on your computer.
:::

<CustomAside icon="star" title="Solution" colour="green">
Leave the datasets on the HPC system and analyze the data remotely. **Don't worry**, you won't have to give up to your preferred visualization software; most likely you will only need to learn a new way to use it. **Keep in mind that**:
1. Compute nodes have **WAY MORE** memory than your local machine will ever have.
2. The **dataset can be analyzed in parallel** by spreading the memory accross multiple processors (just like we ran the simulation in parallel).
3. The **rendering will be mush faster** with the added benefit of using GPUs on the cluster.
</CustomAside>

In this section the goal is to present a wide variety of opportunities offered by the HPC system in terms of visualization. This is not, BY ANY MEANS, a complete and conclusive list, and the students are also given external resources to learn more. Here we will present three common types of visualization on the remote HPC system:
1. **Batch visualization**
2. **Interactive remote desktop visualization**
3. **Interactive client-server visualization**

### Batch visualization
This is probably the easiest and most effective way of visualizing your data on the cluster. If you have used Matplotlib before, or other python libraries for visualiztion, you probably know that this tools (`plt.show()`) open a separate window with the figure we have generated. By now you might think that this is the **standard** approach of data visualization, but **you would be wrong**.

:::note[Batch = NO screen interacion]
Batch visualization has no GUI interaction. The step we follow are:
1. Write your visualization algorithm is a precompiled script.
2. Use the command line to run the script **interactively** or as a SLURM job (reccommended for heavy rendering jobs).
:::

<CustomAside icon="star" title="PROs of Batch visualization" colour="green">
1. Very easy to automate tasks.
2. Well organized and documented workflow, which improves reproducibility.
3. Allows visualization without any GUI elements.
4. Many open-source tools with scripting interfaces:
    - **Python libraries** (e.g. [Matplotlib](https://matplotlib.org/stable/))
    - Some **domain-specific packages** support scripting (e.g. Visual Molecular Dynamics ([VMD](https://www.ks.uiuc.edu/Research/vmd/)) provides python interface)
    - Common **general purpose tools** provide scripting interfaces (e.g. [ParaView](https://www.paraview.org/), [VTK](https://vtk.org/), [Blender](https://www.blender.org/))
</CustomAside>


:::caution[Disclaimer]
Below we will use **Python** and **ParaView** to carry out examples, but the same process will apply to any other programming language or general purpose tools.
:::

The first step to use Python as out batch visualization tool is to install all the required Python packages into a **virtual environment** on the cluster that can be loaded at our convenience. Upon loggin into Graham or any other HPC system type:
```bash title='Create virtual environment'
[username@gra-login1 ~]$ module load python/3.10.13 mpi4py
[username@gra-login1 ~]$ virtualenv --no-download ~/theshire    # install python pckgs into your ~/theshire
[username@gra-login1 ~]$ source ~/theshire/bin/activate
(theshire) [username@gra-login1 ~]$ pip install --no-index --upgrade pip
(theshire) [username@gra-login1 ~]$ pip install --no-index matplotlib numpy plotly h5py
(theshire) [username@gra-login1 ~]$ pip install yt
(theshire) [username@gra-login1 ~]$ deactivate
[username@gra-login1 ~]$
```
Once the environment has been created, it can be `sourced` anytime and from any location on the cluster. The reccommended post processing workflow would be:
```bash title='Virtual environment workflow'
[username@gra-login1 ~]$ source ~/theshire/bin/activate
(theshire) [username@gra-login1 ~]$ python my-post-pro-routine.py
(theshire) [username@gra-login1 ~]$ python my-other-post-pro-routine.py
(theshire) [username@gra-login1 ~]$ deactivate
[username@gra-login1 ~]$
```

Let's take one of the many [examples](https://matplotlib.org/stable/gallery/index.html) available on the Matplotlib web page, and let's use it to generate a PNG figure on the cluster. 

```python title='tri-colour.py' {5} {30-34}
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.tri as tri
mpl.use('Agg')                 # This enables PNG backend
 
# First create the x and y coordinates of the points.
n_angles = 48
n_radii = 8
min_radius = 0.25
radii = np.linspace(min_radius, 0.95, n_radii)
 
angles = np.linspace(0, 2 * np.pi, n_angles, endpoint=False)
angles = np.repeat(angles[..., np.newaxis], n_radii, axis=1)
angles[:, 1::2] += np.pi / n_angles
 
x = (radii * np.cos(angles)).flatten()
y = (radii * np.sin(angles)).flatten()
z = (np.cos(radii) * np.cos(3 * angles)).flatten()
 
# Create the Triangulation; no triangles so Delaunay triangulation created.
triang = tri.Triangulation(x, y)
 
# Mask off unwanted triangles.
triang.set_mask(np.hypot(x[triang.triangles].mean(axis=1),
                          y[triang.triangles].mean(axis=1))
                 < min_radius)
 
# Plotting
plt.figure(figsize=(8,8))
plt.tripcolor(triang,z,shading='flat')
plt.title('Tripcolor test',fontdict={'fontsize' : 15})
plt.savefig('tripcolor.png')
```

Where the only modifications we have made are the highlighted lines in the code to (i) allow PNG backend, and (ii) generate and save the figure as PNG file. The script can now be run in sevrral ways (as seen in [section 1](https://arc4cfd.github.io/section1/outline/) of this course):
1. Directly on the login node (not advised).
2. A an interactive job on the compute node(s).
3. As a bacth job submitted to SLURM.

![Tripcolor plot.](../../../assets/figs_section2/tripcolor.png "Tripcolor.")
        <Caption>Output of the tri-colour.py script.</Caption>

### Interactive remote desktop visualization


Explain common ways of visualizing the flow: [TO DO]
1. Download data on local machine.
3. Use VDI nodes for flow visualization.
2. [parallel/remote  visualization](https://docs.scinet.utoronto.ca/index.php/Visualization)

## Common 'a posteriori' analysis techniques in CFD (JPH: please focus on the HPC aspects, not the turbulence aspects)
Explain the most important type of 'a posteriori' analysis [TO DO]. 
1. Instantaneous flow.
2. Long time average
3. Phase average
4. First order statistics
5. Second order statistics

## Grid convergence and code validation 
Explain the importance of a grid sensitivity study and how to carry it out [TO DO]

## Data transfer
Considerations/ Globus etc