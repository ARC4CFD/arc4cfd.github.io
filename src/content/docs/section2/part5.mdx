---
title: Optimizing CFD for HPC
---
:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Run a comprehensive scaling analysis on your problem
2. Understand the compiling and profiling strategies to optimize HPC
3. Determine parameters that modify the balancing of computational loads among workers
4. Optimize CFD simulations on HPC system 
:::
import Box from '../../../components/Box.astro';
import Caption from '../../../components/Caption.astro';
import CustomAside from '../../../components/CustomAside.astro';
import MultipleChoice from '../../../components/MultipleChoice.astro';
import Option from '../../../components/Option.astro';
import Spoiler from '../../../components/Spoiler.astro';
import CodeFetch from '../../../components/CodeFetch.astro';
import Gif from '../../../components/Gif.astro'


## Optimizing CFD simulations for HPC: Overview
In the previous classes, we planned the simulations, estimated the HPC costs, and pre-processed the files for the simulation. Now, we will turn our attention to optimizing the usage of HPC resources. In this section, for a given CFD simulation and code, the objective is to most efficiently utilize the available HPC ressources. We will assume an end-user approach to the CFD code usage, therefore we won't cover topics that would force us to look into code.

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_workflow_optimal.png "Process simulations of the CFD simulation")



## Assessing scalability of the code
To optimally utilize HPC ressources, the CFD solver must be parallilezable and scalable. That is to say, by increasing the number of compute cores, we reduce the overall wall clock time for a given problem. This speed-up is achieve if each processor is able to complete computational tasks independently from the other. To quantify the parallel performance of a CFD problem on a CFD code, we will define two types of scaling: **strong scaling** and **weak scaling**. Each are presented in this section.


### Strong scaling
Strong scaling, also refered to as Amdahl's law, quantifies the expected speedup with increasing number of computational processors  for a **fixed-sized computational task**. For a fixed task, such as a CFD simulation, increasing the number of parallel processes results in a decrease in the workload per processor, thus, should lead to a reduction in wall clock time. Eventually, as the number of processors increase and the per-processor workload shrinks, the communication overhead between the processors  will impact the strong scaling. The strong scaling is particularly useful for compute-bound processes (CPU-bound), which are typical of most CFD software.

 Many tasks can be divided among processors to reduce the overall computational cost of the simulations, yet some task, let's call them 'housekeeping' tasks, cannot be effectively parallelized.  In CFD codes, 'housekeeping' task may be tied to reading input files or allocating variables, which usually is most important in the initialization of the simulation. For most large scale simulations, these poorly parallelizeable tasks represent only a minimal amount of the total computational cost (it's usually limited to the initialization).  Let's assume a CFD simulation is parallizeable with minimal serial housekeeping tasks the speedup can be compute as the ratio of the time it takes (in seconds) a specific task on 1 processor ($t(1)$) to time of the same task on $n$ processors ($t(n)$):
$$
    S_n=\frac{T(1)}{T(n)}
$$
Ideal parallelizability would imply that doubling the number of processors would halve the computational wall time, or: $T_1= n T_n$. This would correspond to 100\% parallel efficiency of the code, where the parallel efficiency is defined as:
$$
    \eta = \frac{n T(1)}{T(n)}
$$
Parallel efficiency [to do]

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_StrongScaling.png "Strong scaling of a CFD code.")
<Caption>Strong scaling of a CFD code. </Caption>

Although theoretically possible, superlinear speed up is usually rarely observed in CFD simulations, especially as 
https://ieeexplore.ieee.org/document/7733347



###  Weak scaling
Weak scaling defines the computational efficiency of a **scaled problem size**  with number of processors. Weak scaling evaluates a constant per processor workload, thus provides an estimate of the size of the computational task on a larger HPC system for the same wall clock time. Weak scaling is often refered to as Gustafson's law and is a perticularly useful to evaluate memory-limited applications; embarassingly parrellel problems tend to have near perfect weak scaling capabilities.  As weak scaling involves increasing the computational tasks with this number of processors, this implies scaling the number of grid points with the number of processors, thus represents a more involved scaling test compared to strong scaling.

For a problem size of $M$ computed on $N$ processors, the time to complete a task is $t(M,N)$. With these definitions, we can define the weak scaling speedup as:
$$
    \text{weak speedup} =\frac{t(M,1)}{t(N\times M,N)} 
$$
It's clear that the size of the computational task on the denomintor ($N\times M$) increases with the number processors, thus maintaining a constant workload on each processor. As the number of processes increase, the communication overhead increases, reducting the weak scaling speedup. For geometrically complex problems, weak scaling is challenging as it requires a new mesh for each number of processors. For a 2D structured mesh, we can, for example, define $M=64^2$ to be run on 1 processor. On four processors, each processor would run  $M=64^2$ but the total CFD problem would be  $4\times M= 128^2$; on 16 processors, each processor still $M=64^2$ but the total problem size is $16\times M= 256^2$, and so on.



![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_WeakScaling.png "Competing aspects in setting up CFD simulations")
<Caption>Weak scaling of a CFD code. </Caption>

<CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/openfoam/Tutorials/scalingTests/strongScalingTest/runStrongScalingTest.sh' lang='bash' meta="title='strongscalingTest.sh'" />




### Scaling tests for CFD
The scaling tests are an **essential** component to running large scale simulations on HPC systems. It's important that the scaling test will naturally depend on the CFD code but also, very importantly, will depend on the simulation. For example, if you use a dynamic inflow boundary condition (injection of turbulence at the inlet), the local overhead in computing the inflow variable may result in a slowdown due to the processor waiting on others. As a result, a same CFD code with different operating conditions may show different scaling properties.  Similar, a same code run on two different HPC systems may show different scaling results. A code performance is dependent on the architecture of the HPC systeme.

:::tip[Tip]
It's important to conduct scaling tests on: 
1. problems that are nearly identical to the planned production runs;
2. the HPC system that you plan on using.
:::


In order to quantify the scaling, we must have tools to measure.  Most commonly, we will use the wall-clock time in scaling test but other quantifiable metrics may also be used. The Compute Ontario HPC systems have the following command that can be used to compute the wall clock time for a given process:
```bash
[username@nia0144 ~]$ /bin/time SU2_CFD MyInput.cfg
```


The following list are additional considerations you want to have when conducting scaling tests on CFD code:
1. Define a consist metric to evaluate the compute time (e.g. wall clock time to run 100 time steps);
2. Avoid defining times that are not consistent or dependent on the simulation (e.g. computing wall clock time to run 1 s of simulation time)
3. Select the metric such that you minimize the importance of the serial housekeeping tasks: for example if you only compute 5 time steps, yet the initialization takes up about 25\% of the total, you will have an incorrect assesment of the scaling;
4. Scaling is solver and hardware specific. A scaling test on different architecture will give different results.
5. Use a simulation that is most representative of the actual run you plan on conducting.
6. Ideally, run multiple independent runs per job size and average the results.
7. Try to run on an interactive/debug node.





[test](https://hpc-wiki.info/hpc/Scaling_tests)


<Box iconName='exercise'>
## EXAMPLE: Scaling tests

[SCALING TESTS HERE]

Step by step process for scaling test


Show scaling on Graham, niagara, etc

Show difference in scaling

</Box>

https://su2foundation.org/wp-content/uploads/2020/06/Gomes.pdf



## Assessing the optimal HPC system for a given code
The roofline model is a simple representation of arithmetic intensity  versus performance of a code, and helps to asses the theoretical limits based on the systems architecture and memory characteristics. There are two limiting factors to the performance of a code, namely limitations on the:
1.  CPU peak performance (compute-bound)
2. RAM memory bandwidth (memory-bound)

The roofline 

The roofline model can be mathematically defined as:
$$
    P_{max}=\min(P_{peak},I*b_{max})
$$
where $P_{peak}$ is the hardware peak performance (FLOPS) and  is the peak bandwidth $b_{max}$. The $I$ is the algortihmic intensity and is code dependent. Most CFD codes have a high algorithmic intensity, for this reason, they are most of the time, compute-bound. Optimally, we would select a system which falls at the ridge point where the memory and computational limits intersect.

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_Roofline.png "Competing aspects in setting up CFD simulations")

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_RooflineCompare.png "Competing aspects in setting up CFD simulations")



## Stricking a balance: HPC vs accuracy
In many explicit time-avancement simulations, the time step is dictated by the maximum CFL number over the entire domain. As the domain typically has small cells near the walls and much larger cells further away, the time advancement is often constrained by a few cells that impose a much smaller time step than needed everywhere else. It's not uncommon to see:

Courant Number mean: 0.00404550158114 max: 2.69641684221

That means that the AVERAGE CFL number is 675 times smaller than the cell size that is constraining the simulation. This can help us identify potential speed-up opportunities (although we be be congnisant that it may come at the expense of accuracy!). For this example, we have a couple options to explore, we can:

- **Investigate and adjust the mesh**: Looking at the preliminary results, we can identify the regions of high CFL number that are constraining the time advacement. If the fine resolution is not needed at this location, then we can adjust the mesh accordingly with the realization that increasing by half the characteristic mesh size will effectively halve the simulation time (big savings!). In most cases, the fine mesh resolution is needed to resolve local flow gradient. As such the user must investigate the trade-off between accuracy and computational speed.
- **Consider other modeling strategies**: Sometimes, it is not possible to increase the mesh size without greatly affecting the solution accuracy. In this case, the user may consider to apply wall-modeling strategies which have the benefit of greatly reducing computational costs and possibly have better control on the error of the simulation.





[test](https://www.cse-lab.ethz.ch/wp-content/uploads/2022/09/Principles-of-HPC.pdf)
[test](https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf)
[test](https://www.cines.fr/wp-content/uploads/2014/10/lesson2_slides.pdf)


## Achieving  load balance parallel computations
For the optimal use of HPC ressources, the parallelized tasks should be equally spread out among all processors. Imbalances in the workload, can inneficiencies that result in some processors waiting for others to finish. The load imbalance may be particularly accute in CFD as HPC systems are increasingly heterogeneous and dynamic workload (through, for example, adaptive mesh refinement) 
ref ionescu_high-performance_2018

How do we effectively divide the computational tasks among the processors

[test](https://2018.foam-iberia.eu/slides/FOAM-IBERIA_RobRibeiro.pdf)




Processor 39
    Number of cells = 517259
    Number of points = 545608
    Number of faces shared with processor 19 = 5405
    Number of faces shared with processor 38 = 20216
    Number of processor patches = 2
    Number of processor faces = 25621
    Number of boundary faces = 30421


Number of processor faces = 957287
Max number of cells = 517259 (9.66634671059e-05% above average 517258.5)
Max number of processor patches = 5 (31.5789473684% above average 3.8)
Max number of faces between processors = 67248 (40.4970505188% above average 47864.35)


Number of processor faces = 1841355
Max number of cells = 258639 (0.00376987521713% above average 258629.25)
Max number of processor patches = 5 (26.582278481% above average 3.95)
Max number of faces between processors = 65100 (41.4175973672% above average 46033.875)


### Communication among processors
Most modern CFD codes primarily use multiple MPI processes on different physical CPU cores which are connected. As seen earlier, the inter-processor communication is often the bottleneck which limits the interprocessor communication.

In CFD computations, communication is necessary to compute the flux accross physical MPI processes as well as averaging other operations that require information exchange (e.g. fast fourier transforms). The deecomposition of the CFD domain among the various processors is not unique and can impact the communication, and thus scalability of the code. In the illustrative example shown in Figure (ref fig:sec2_Parallelization), an $N\times M$ CFD domain is decomposed either in four blocks of either $N\times M/4$ or  $N/2\times M/2$ on two dual core processors. As the communication between cores has typically more latency than among two processors on the same chip. The overall communication will be more efficient with the $N\times M/4$ decomposition. It is easy comprehend how this communication problem is exacerbated when discussing high number of cores for a 3D problem.



![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_Parallelization.png "Competing aspects in setting up CFD simulations")


ref garcia-gasulla_generic_2020



:::note[Important to keep in mind]
>> cat /proc/cpuinfo

processor	: 77
vendor_id	: GenuineIntel
cpu family	: 6
model		: 85
model name	: Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz
stepping	: 4
microcode	: 0x2007006
cpu MHz		: 3100.195
cache size	: 28160 KB
physical id	: 1
siblings	: 40
core id		: 26
cpu cores	: 20
apicid		: 117
initial apicid	: 117
fpu		: yes
fpu_exception	: yes
cpuid level	: 22
wp		: yes
:::




## Compiling and profiling CFD codes
Profiling CFD codes can help understand the bottlenecks in the code and provide hints about potential improvements
export WM_COMPILE_OPTION=Prof

https://www.nhr.kit.edu/userdocs/horeka/profiling_gprof/
https://meywang.wordpress.com/2011/07/29/openfoam-profiling/
```bash
[username@gra-login1 ~]$ diskusage_report




### Problem set
You are trying to estimate the size of the CFD task you can run on a larger system, what scaling test should you run?

ANS: Weak scaling test.



{/*
*[test](https://www.cse-lab.ethz.ch/wp-content/uploads/2022/09/Principles-of-HPC.pdf)
[test](https://www.nersc.gov/assets/Uploads/Tutorial-ISC2019-Intro-v2.pdf)
[test](https://www.scientific-computing.com/hpc2018-19/the-roofline-model)
[test](https://www.youtube.com/watch?v=IrkNZG8MJ64)
[test](https://inria.hal.science/hal-03207431/document)


*/}



