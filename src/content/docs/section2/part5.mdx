---
title: Optimizing CFD for HPC
---

:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Run a comprehensive scaling analysis on your problem
2. Understand the compiling and profiling strategies to optimize HPC
3. Determine parameters that modify the balancing of computational loads among workers
4. Optimize CFD simulations on HPC system 
:::




In the previous section, we developed a systematic approach to construct a CFD problem that aligns with the underlying scientific question that we seek to answer, conducted an a priori estimate of the computational expense, and pre-processed the simulation. 

Given a mesh, setup, and operating conditions, the objective of this section is to efficiently utilize the available HPC ressources.





For a given CFD simulation, how can we determine the best system to use?

[test](https://www.cse-lab.ethz.ch/wp-content/uploads/2022/09/Principles-of-HPC.pdf)
[test](https://www.nersc.gov/assets/Uploads/Tutorial-ISC2019-Intro-v2.pdf)
[test](https://www.scientific-computing.com/hpc2018-19/the-roofline-model)
[test](https://www.youtube.com/watch?v=IrkNZG8MJ64)
[test](https://inria.hal.science/hal-03207431/document)



## Compiling and profiling CFD codes

[test](https://docs.scinet.utoronto.ca/index.php/Introduction_To_Performance#gprof_.28profiling:_everywhere.29)




## Why is scaling important? 
We seek to best utilize the computational architechture for our CFD problem. The 


## Assessing scalability of the code
To optimally utilize HPC ressources, the CFD solver must be parallilezable and scalable. That is to say, by increasing the number of cores, we reduce the overall wall clock time for a given problem. To quantify the parallel performance, the computational speedup. There are two types of scaling that help to quantify the parallel performance of a CFD code on specific HPC hardware: **strong scaling** and **weak scaling**.


### Strong scaling
Strong scaling, also refered to as Amdahl's law, quantifies the expected speedup with increasing number of computational processors  for a fixed-size computational taks. For a fixed task, such as a CFD simulation, increasing the number of parallel processes results in a decrease in the workload per processor, thus a reduction in wall clock time. Eventually, as the number of processors increase and the per-processor workload shrinks, the communication overhead will impact the strong scaling. The strong scaling is particularly useful for compute-bound processes (CPU-bound), which are typical of most CFD software.

 Many tasks can be divided among processors to reduce the overall computational cost of the simulations, yet some task, let's call them 'housekeeping' tasks, cannot be effectively parallelized.  In CFD codes, 'housekeeping' task may be tied to reading input files or allocating variables, which usually is most important in the initialization of the simulation. For most simulations, these poorly parallelizeable tasks represent a minimal amount of the total computational cost.  Let's assume a CFD simulation is parallizeable with minimal tasks serial housekeeping task, the speedup can be compute as the ratio of the time it takes (in seconds) a specific task on 1 processor ($t(1)$) to time of the same task on $n$ processors ($t(n)$):
$$
    S_n=\frac{T(1)}{T(n)}
$$
Ideal parallelizability would imply that doubling the number of processors would halve the computational wall time, or: $T_1= n T_n$. This would correspond to 100\% parallel efficiency of the code, where the parallel efficiency is defined as:
$$
    \eta = \frac{n T(1)}{T(n)}
$$
Parallel efficiency 

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_StrongScaling.svg "Competing aspects in setting up CFD simulations")



Although theoretically possible, superlinear speed up is usually rarely observed in CFD simulations, especially as 
https://ieeexplore.ieee.org/document/7733347



###  Weak scaling
Weak scaling defines the computational efficiency of a scaled problem size  with number of processors. Weak scaling evaluates a constant per processor workload, thus provides an estimate of the size of the computational task on a larger HPC system for the same wall clock time. Weak scaling is often refered to as Gustafson's law and is a perticularly useful to evaluate memory-limited applications; embarassingly parrellel problems tend to have near perfect weak scaling capabilities.  As weak scaling involves increasing the computational tasks with this number of processors, this implies scaling the number of grid points with the number of processors, thus represents a more involved scaling test compared to strong scaling.

For a problem size of $M$ computed on $N$ processors, the time to complete a task is $t(M,N)$. With these definitions, we can define the weak scaling speedup as:
$$
    \text{weak speedup} =\frac{t(M,1)}{t(N\times M,N)} 
$$
It's clear that the size of the computational task on the denomintor ($N\times M$) increases with the number processors, thus maintaining a constant workload on each processor. As the number of processes increase, the communication overhead increases, reducting the weak scaling speedup. For geometrically complex problems, weak scaling is challenging as it requires a new mesh for each number of processors. For a 2D structured mesh, we can, for example, define $M=64^2$ to be run on 1 processor. On four processors, each processor would run  $M=64^2$ but the total CFD problem would be  $4\times M= 128^2$; on 16 processors, each processor still $M=64^2$ but the total problem size is $16\times M= 256^2$, and so on.




![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_WeakScaling.svg "Competing aspects in setting up CFD simulations")



## Scaling tests for CFD
The scaling tests 
1. Use wall clock times or any equivalent metric to compute the time
2. Define a consist metric to evaluate time (e.g. time to run 100 time steps). Avoid defining times that are not consistent or dependent on the simulation (e.g. 1s of simulation time)
3. Select the metric such that you minimize the importance of the serial housekeeping tasks: for example if you only compute 5 time steps, yet the initialization takes up about 25\% of the total 
4. Scaling is solver and hardware specific. A scaling test on different architecture will give different results.
5. Use a simulation that is most representative of the actual run you plan on conducting.
6. Ideally, run multiple independent runs per job size and average the results.






To be valuable, the scaling test should be conducted on a the same system and similar problem
[test](https://hpc-wiki.info/hpc/Scaling_tests)




