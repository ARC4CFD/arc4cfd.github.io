---
title: 'Running simulations on HPC'
---

import Box from '../../../components/Box.astro';
import Caption from '../../../components/Caption.astro';
import Option from '../../../components/Option.astro';
import CustomAside from '../../../components/CustomAside.astro';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import CodeFetch from '../../../components/CodeFetch.astro';
import MultipleChoice from '../../../components/MultipleChoice.astro';

:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Organize and plan simulation files on the cluster
2. Run the simulation and optimize
3. Carry out a runtime analysis of flow parameters
:::

## Running CFD simulations on HPC: Overview

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_workflow_PROC.png "Process simulations of the CFD simulation")



{/*[test](https://www.grc.nasa.gov/WWW/wind/valid/tutorial/spatconv.html)
[test](https://curiosityfluids.com/2016/09/09/establishing-grid-convergence/)*/}
{/*![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_IO.svg "Competing aspects in setting up CFD simulations")*/}

We are now ready to start a large-scale CFD simulation. As mentioned in the previous sections, we have chosen a problem (the backward facing step, BFS) that has some very important flow features, but most importantly it's a case that has been extensively analyzed in the literature both numerically [Li et al. 1997](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/direct-numerical-simulation-of-turbulent-flow-over-a-backwardfacing-step/645D21758E8F74568008899C17B12ADD), and experimentally [Jovic and Driver 1994](https://ntrs.nasa.gov/api/citations/19940028784/downloads/19940028784.pdf). The purpose of this section is to standardize the workflow of **setting up**, **running**, and **monitoring** a large-scale CFD simulation on a remote HPC system. Buckle up!

## Organize simulation files on the cluster
The first and **most important** step is to well organize the simulation files on the cluster. During your career as a researcher you will probably end up running a **LARGE** number of numerical simulations, and a well planned bookkeeping can save you **A LOT** of time.

:::caution[Disclaimer]
1. Although, all examples and simulations will be carried out using the University of Waterloo cluster **Graham**, the same process will apply to any other Compute Ontario HPC systems.
2. The workflow presented here **is NOT** a *set in stone* rule, but rather a **suggestion** for any user of HPC resources.
:::

If you are following along with the course, you probably have already cloned all the example files and GitHub repository on the Cluster, if not please go back and check [section 2.1](https://arc4cfd.github.io/section2/part1/). The question now is: **how do we organize our simulation files?**

To answer this question, let's first figure out what options we have. Upon loggin into Graham, type:
```bash
[username@gra-login1 ~]$ diskusage_report
```
This command will check available disk space and the current disk utilization on our **personal** and **group** profiles. The output will look something like:
```bash
[username@gra-login1 ~]$ 
                             Description                 Space             # of files
                     /home (user username)              23G/50G             112/500k
                  /scratch (user username)            6633G/20T            14k/1000k
                 /project (group username)              0/2048k               0/1025
               /project (group def-piname)           292M/1000G             294/505k
            /project (group rrg-piname-ac)             49T/100T            112k/505k
```
Where `username` refers to your personal space, while `piname` to your group (or principal investigator) profile. `/home` has a capacity of 50GB and, as we mentioned earlier, is not the recommended place to store simulation results or data. `/home` is suitable for code development and version control. `/project (group rrg-piname-ac)` (in this case) is the largest directory on the cluster, it's linked to your principal investigator account, and is the place where **FINAL** data should be stored long-term. Finally, `/scratch` is the second largest directory, it's connected to the single user, and as mentioned in [section 2.1](https://arc4cfd.github.io/section2/part1/), it's the right place to **set up and run** your simulations.

:::danger[Final Warning]
Important files must be copied off `/scratch` regularly since they are not backed up and older files are subject to purging!
:::

Following this approach, one can clone from the course [GitHub repository](https://github.com/ARC4CFD/arc4cfd/tree/master) both the **OpenFOAM** and **SU2** BFS examples to the remote `home` into the respective case directory:

<Tabs group="tab-group">
    <TabItem label="/home/bfs_openfoam">
    ```bash
    [username@gra-login1 ~]$ ls
    README              case                movie.ogv           run.sh
    bfs_Umag.pvsm       mesh                run_jobscript.sh
    ```
    </TabItem>
    <TabItem label="/home/bfs_su2">
    ```bash
    [username@gra-login1 ~]$ ls
    Coarse                                       README
    Fine                                         Turbulent-flow-over-Backward-facing-step.pdf
    Intermediate                                 su2job.sh
    ```
    </TabItem>
</Tabs>

### Create the Run directory
Now that you have cloned the required tools in your `/home`, think of them as the **SOURCE** code of your CFD tools. Any modifications you want to do on the source should happen in `/home`, as it's your own personal space, and nothing will be purged from here. However:

:::caution[Caution]
`/home` is not the place **to run** your simulation. Depending on the size of your mesh, and the number of snapshots saved, you will reach the **quota limit** of 50GB very soon. 
:::

For this reson, once you are happy with the changes implemented on the source code, you should **copy** the code into a **Run directory** in `/scratch`. In this case no changes were required in the source files, therefore we can directly:
<Tabs group="tab-group">
    <TabItem label="/scratch/01_BFS_openFOAM">
    ```bash
    [username@gra-login1 ~/home/bfs_openfoam]$ cp -r * ./scratch/01_BFS_openFOAM
    ```
    </TabItem>
    <TabItem label="/scratch/02_BFS_SU2">
    ```bash
    [username@gra-login1 ~/home/bfs_su2]$ cp -r * ./scratch/02_BFS_SU2
    ```
    </TabItem>
</Tabs>

Don't underestimate the importance of **name convention** for files and directory. We reccommend to find your own preferred system, and be consistent with it. **At this point we are ready to setup the simulation.**


### Setting up simulation parameters
All the simulation files have already been prepared, and here we highlight only the most important steps to follow before running the simulation:

1. **Mesh size**: the first and most important choice is the **number of points**. We prepared the case so that students can run several **meshes**. Here, however, for semplicity's sake, we only follow the example of the coarsest mesh of $237000$ grid points (named `bfs_200k_DDES`).

2. **Time step size**: the choice of the grid size directly impact the maximum time step size one can use due to the CFL stability condition ([section 2.3](https://arc4cfd.github.io/section2/part3/)). For the chosen case (of $\approx 200000$ points) the CFL stability condition requires: $\Delta_t=10^{-4}$.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change deltaT to 1e-4;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

3. **Domain decomposition**: after performing the scaling test for a given mesh ([section 2.5](https://arc4cfd.github.io/section2/part5/)), we know how many processors we should use to optimize the CFD workflow.
<details>
    <summary>OpenFOAM</summary>
    The user should modify the `numberOfSubdomains` entry in the `case/system/decomposeParDict` file. In this example we use 64 processors.
    ```bash
    vim case/system/decomposeParDict
    # change numberOfSubdomains to 64;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

4. **Simulation End Time**: especially when turbulence or geometric non-homogeneity are present, it's important to run the simulation long enough to let the flow properly develop in the computational domain. This phase is usually referred to as **the transient** phase. Once the flow is fully developed, a **steady state** is reached and the flow is referred to as **statistically stationary**. As mentioned in [section 2.3](https://arc4cfd.github.io/section2/part3/) estimating the time required to reach a steady-state is very difficult and is case-dependent. A reasonably good measure to get a rough estimate is the Large Eddy Turn-Over Time (LETOT). One LETOT is the time required for the largest eddy to leave the domain. **In this example we choose the End Time to correspond to $\approx 10$ LETOTs**.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change endTime to 0.5;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

5. **Snapshot time interval**: once again, when turbulence governs the physics of the flow we use statistical methods to process and analyze the results. For this reason one should collect a large sample of **flow realization** or snapshots in order to perform post-processing analysis and compute flow statistics. The interval between two successive snapshots should not be too large (as they will be uncorrelated) nor too small (as they will be too much correlated). One can always refer to the LETOT and choose a percetage of that as the sampling interval.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change writeInterval to 20;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

After all flow and simulation paramters have been set, **we are now ready to run the simulation**. 

## Run a large-scale CFD simulation
As previously seen in [section 1.5](https://arc4cfd.github.io/section1/part5/) of this course when solving the 2D Poisson equation, the are 2 common ways of running large-scale simulations on the cluster: (i) an interacive session by loggin into the compute nodes directly, and (ii) submitting a batch job to SLURM. For a very small simulation such as the 2D Poisson there is not much difference (in terms of time) between the 2 methods, however when the simulation becomes more involved, disadvantages of the former and advantages of the latter become more evident.

### Running in interactive mode
Now that we copied the **Source** code from our `/home` directory into `/scratch`, our goal is to run 3 simulations starting from a coarse mesh ($\approx 200000$ points) to a fine mesh (of about $800000$ grid points). As we shall see in a later section, there is a reason behind this choice, but for now let's follow the steps towards running the simulation:

<Tabs group="tab-group">
    <TabItem label="OpenFOAM steps">
    1. **Allocate required HPC resources**:
    ```bash
    salloc -n 64 --time=10:00:0 --mem-per-cpu=3g --account=account-name
    ```
    2. **Create the sub-case directory** `bfs_200k_DDES` within the main case directory:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ cp -r * ./case/* ./bfs_200k_DDES
    ```
    3. **Generate mesh** from file using the `gmsh` utility:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gmsh mesh/bfs_200k.geo -3 -o mesh/bfs_200k.msh -format msh2
    ```
    4. **Convert mesh to OpenFOAM format** and **modify boundary file to reflect boundary conditions**:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gmshToFoam mesh/bfs_200k.msh -case /bfs_200k_DDES
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ cp /bfs_200k_DDES/constant/polyMesh/boundary /bfs_200k_DDES/constant/polyMesh/boundary.old
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i '/physical/d' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/wall_/,/startFace/{s/patch/wall/}" /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/top/,/startFace/{s/patch/symmetryPlane/}" /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/front/,/startFace/{s/patch/cyclic/}" /bfs_200k_DDES/constant/polyMesh boundary                                        
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/back/,/startFace/{s/patch/cyclic/}" /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/front/,/}/{/startFace .*/a'"\\\tneighbourPatch  back;" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/back/,/}/{/startFace .*/a'"\\\tneighbourPatch  front;" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/cyclic/,/nFaces/{/type .*/a'"\\\tinGroups        1(cyclic);" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/wall_/,/}/{/type .*/a'"\\\tinGroups        1(wall);" -e '}' /bfs_200k_DDES/constant/polyMesh/boundary
    ```

    5. **Start the simulation**:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ cd ./bfs_200k_DDES
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ ./Allrun
    ```
    Where the `Allrun` script perfoms some very important opearions. Among them:
    ```bash
    cp -r 0.orig 0                # initialize the flow
    runApplication decomposePar   # decomposed the domain based on the # of procs.
    runParallel $(getApplication) # run the application in parallel
    runApplication reconstructPar # after simulation is done join processors into single file
    rm -rf processor*             # remove all the single processors directories
    ```
    After **step 5** is completed, you should see the simulation starting on the terminal:
    ```bash
    Running decomposePar on /home/ambrox/scratch/BFS_OpenFOAM/bfs_200k_DDES
    Running pimpleFoam in parallel on /home/ambrox/scratch/BFS_OpenFOAM/bfs_200k_DDES using 64 processes
    ```
    At this point the terminal window *hangs* while the simulation runs. If the terminal window is closed the simulation **stops**.
    </TabItem>
    <TabItem label="SU2 steps">
    ```bash
    [username@gra-login1 ~/scratch/02_BFS_SU2]$ cp -r * ./case/* ./bfs_200k_DDES
    ```
    </TabItem>
</Tabs>

Here are some *PROs* and *CONs* of running a large scale simulation in interacive mode:
<CustomAside icon="star" title="PROs" colour="green">
1. Easier to set up.
2. Easier to run. 
</CustomAside>

<CustomAside icon="warning" title="CONs" colour="red">
1. **Not suited** for **LONG** jobs as the terminal window must stay open and the workstation on.
2. **Not suited** when running several simulations. 
</CustomAside>

### Submitting a batch script
In most of the cases, a CFD engineer, or a PhD student would need to run a large series of numerical simulations, might they involve different geometries, boundary conditions, mesh sizes etc. As you might have already guessed, running $N$ simulations in interactive mode not only becomes a tedious repetition of the same 5 steps (in the OpenFOAM case), but it also increases the chances of making mistakes along the way. 

A **very good** idea in HPC, when dealing with repeatitive tasks is the concept of **automation**. In this case, for instance, we could include Steps 1-5 in a single file `run.sh` to be run in interacive mode, or **even better** in a batch job script `run_jobscript.sh` to submit to the job scheduler. Both files are included in the GitHub repository, and shown below:

<details>
    <summary>run.sh</summary>
    <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/openfoam/Tutorials/BFS_OpenFOAM/run.sh' lang='bash' meta="title='run.sh' " />
</details>
<details>
    <summary>run_jobscript.sh</summary>
    <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/openfoam/Tutorials/BFS_OpenFOAM/run_jobscript.sh' lang='bash' meta="title='run_jobscript.sh' " />
</details>

The command to submit the bath script would be simply:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sbatch run_jobscript.sh
Submitted batch job 26236582 
```

<Box iconName='quiz'>
### Problem 1
Run the numerical simulation of the same backward facing step flow for the mesh containing $\approx 400000$ grid points. 

What would be the time step size $\Delta t$ required by the CFL stability condition?
<MultipleChoice>
    <Option>
      The same
    </Option>
    <Option  isCorrect>
      $4\times 10^{-5}$   
    </Option>
    <Option>
      $2\times 10^{-5}$   
    </Option>
</MultipleChoice>

What would be the writeInterval required to still print results every 2 milliseconds?
<MultipleChoice>
    <Option>
      20
    </Option>
    <Option  isCorrect>
      50   
    </Option>
    <Option>
      40   
    </Option>
</MultipleChoice>
</Box>


## Perform a runtime analysis of the simulation
Once the job is submitted, we should make sure the simulation is running properly. This is done by typing the command:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sq
JOBID     USER      ACCOUNT       NAME    ST  TIME_LEFT   NODES CPUS TRES_PER_N MIN_MEM NODELIST (REASON) 
26236582  username  def-piname  bfs_DDES   R   19:59:46     8    64        N/A    3G    gra[11005,11007,11010,11012-11016] (None)
```

<Tabs group="tab-group">
    <TabItem label="OpenFOAM">
        The code is running, as expected, on 64 processors using 8 nodes. This check however, does not really tell us that **everything is going well** but only that the 64 processes have started and are working at something. The next step would be to check the **log file**. If you recall, with the command `mpirun pimpleFoam -parallel > log.pimpleFoam` in the **run_jobscript.sh** we asked the code to write any output to a log file called `log.pimpleFoam`. If you notice, this file popped up into our case directory (`bfs_200k_DDES`) as soon as the simulation started. 

        Depending on how far along in the simulation you are, the `log.pimplefoam` file might be quite long. To give you a quick run through of how it looks, let's visualize the beginning of it:

        <details>
            <summary>See log.pimpleFoam</summary>
            ```bash
            [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ vim log.pimpleFoam
            Starting time loop

            Courant Number mean: 0.37558691 max: 5.0866216
            Time = 0.0081

            PIMPLE: iteration 1
            DILUPBiCGStab:  Solving for Ux, Initial residual = 0.0543257, Final residual = 0.0030492453, No Iterations 1
            DILUPBiCGStab:  Solving for Uy, Initial residual = 0.019132138, Final residual = 0.0009267347, No Iterations 1
            DILUPBiCGStab:  Solving for Uz, Initial residual = 0.066712166, Final residual = 0.0049700607, No Iterations 1
            GAMG:  Solving for p, Initial residual = 0.98050352, Final residual = 0.021488268, No Iterations 2
            time step continuity errors : sum local = 0.00044866542, global = -1.3391353e-05, cumulative = -1.3391353e-05
            GAMG:  Solving for p, Initial residual = 0.14858043, Final residual = 8.6866034e-07, No Iterations 45
            time step continuity errors : sum local = 3.0582936e-08, global = -3.2758568e-09, cumulative = -1.3394628e-05
            PIMPLE: iteration 2
            DILUPBiCGStab:  Solving for Ux, Initial residual = 0.058760738, Final residual = 0.003117804, No Iterations 1
            DILUPBiCGStab:  Solving for Uy, Initial residual = 0.021669178, Final residual = 0.00078736235, No Iterations 1
            DILUPBiCGStab:  Solving for Uz, Initial residual = 0.12996742, Final residual = 0.010186982, No Iterations 1
            GAMG:  Solving for p, Initial residual = 0.59370611, Final residual = 0.014856221, No Iterations 2
            time step continuity errors : sum local = 0.00034748909, global = 1.1426898e-05, cumulative = -1.9677303e-06
            GAMG:  Solving for p, Initial residual = 0.30867957, Final residual = 9.1670175e-07, No Iterations 49
            time step continuity errors : sum local = 1.3500471e-08, global = 1.43852e-09, cumulative = -1.9662918e-06
            PIMPLE: iteration 3
            DILUPBiCGStab:  Solving for Ux, Initial residual = 0.015980338, Final residual = 0.0006615279, No Iterations 1
            DILUPBiCGStab:  Solving for Uy, Initial residual = 0.0087843757, Final residual = 0.00022240436, No Iterations 1
            DILUPBiCGStab:  Solving for Uz, Initial residual = 0.11693161, Final residual = 0.006951935, No Iterations 1
            GAMG:  Solving for p, Initial residual = 0.65043891, Final residual = 0.010075974, No Iterations 2
            time step continuity errors : sum local = 0.00010854745, global = 2.6004423e-06, cumulative = 6.3415051e-07
            GAMG:  Solving for p, Initial residual = 0.40140073, Final residual = 9.079401e-07, No Iterations 47
            time step continuity errors : sum local = 4.8308146e-09, global = 5.1590399e-10, cumulative = 6.3466642e-07
            ```
        </details>

        Important information to retain from the log file are:
        1. **The time iteration** corresponds to the time integration of the equations of motion mentioned in [section 2.3](https://arc4cfd.github.io/section2/part3/).
        2. **The CFL** or Courant number is displayed at every time stamp.
        3. **Residuals** are shown at each iteration for all velocity components and pressure.
        4. **Local and global** mass conservation is also printed at each iteration.

        These 4 pieces of information are already incredibly useful to understand if the simulation is converging, diverging, if mass is globally conserved, or if there is a problem in the domain. 

        The simulation will be running probably for several hours, and the ideal scenario is that every once in a while we check the behavior of the residuals and mass conservation. As you might guess staring at numbers on the screen is not the best approach and, once again, it is better to adopt an automated mechanism to visualize residuals. This can be done using **gnuplot**, a command-line and GUI program that can generate two- and three-dimensional plots of functions, data, and data fits. Gnuplot is usually present by default in any UNIX system, however to make sure you have it in your profile on the cluster, you can type:
        ```bash
        [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gnuplot

            G N U P L O T
            Version 5.4 patchlevel 2    last modified 2021-06-01 

            Copyright (C) 1986-1993, 1998, 2004, 2007-2021
            Thomas Williams, Colin Kelley and many others

            gnuplot home:     http://www.gnuplot.info
            faq, bugs, etc:   type "help FAQ"
            immediate help:   type "help"  (plot window: hit 'h')

        ```
        If you don't see the gnuplot welcome message, or if the terminal throws you an error, you can load the gnuplot module just like any other module:
        ```bash
        [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ module load gnuplot 
        [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ module save
        ```

        We can now write a simple script to plot residuals during runtime:
        <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/openfoam/Tutorials/BFS_OpenFOAM/plot-residuals' lang='bash' meta="title='plot-residuals' mark={21}" />

        The script above will plot the residuals for all velocity components during the past 40 time steps. The students can modify the `highlighted line` in the script to change the plotting range. The script **MUST BE** located in the same directory of the `log.pimpleFoam`, and to run it simply type:

        ```bash
        [username@gra-login1 ~/scratch/01_BFS_openFOAM/bfs_200k_DDES]$ gnuplot plot-residuals
        ```

        ![Residuals.](../../../assets/figs_section2/residual-plot.png "Residuals.")
        <Caption>Residuals of the three velocity components, $U_x$, $U_y$, and $U_z$.</Caption>


    </TabItem>
    <TabItem label="SU2">
        SU2
    </TabItem>
</Tabs>

## Grid convergence and code validation 
Explain the importance of a grid sensitivity study and how to carry it out [TO DO]

## You might want to think about output files
When running a simulation in parallel it is **CRUCIAL** to think about the impact of the output files on the HPC workflow. Running $N$ simulations without thinking about **I/O** penalty will cause you A LOT of trouble both in terms of bookkeeping but also in terms of disk quota.

### Number of output files
Depending on the CFD tool used, when running a simulation in parallel, we need to remember that the computational domain has been decomposed in $N$ processes. Therefore, if one was to save a snapshot every time step, and to integrate the equations of motion for $N_{t}$ time steps, **every single processor** will output data at **each** timestep. Assuming that the code is outputting 5 variables ($u$,$v$,$w$,$P$, and $\nu_{t}$), this will give rise to $N\times N_{t}\times 5$ files in our `scratch` directory. 

:::danger['Danger']
In simple terms, the coarse simulation of the BFS we have just carried out on 64 processors for about 25000 iterations would generate about 8 million files!! This is why the `time interval` between snapshots should be chosen wisely.
:::

This type of output is known as **parallel output**, and one should always consider to **merge** all processors file at each time stamp after the simulation is done or (if possible) during runtime. This is exacly why the `reconstructPar` was included in the openFOAM batch script file. 

<Box iconName='exercise'>
Even though some CFD tools available will perform this operation by default, it is always a good idea to perform a rough estimate of the number of output files expected from a numerical simulation. Let's consider our BFS simulation integrated for a total time of $T=0.5\,s$, saving snapshots every $2\, ms$, on 64 processors.

1. **Number of files parallel output**:

$$N_{files} = 64\times 250\times 5 = 80000$$

2. **Number of files merged output**:

$$N_{files} = 250\times 5 = 1250$$

</Box>

### Size of output files
Some thought should also be given to the size of the output files. Without going too much in details, in HPC we have two possible output styles:

1. **Binary**: as mentioned in [section 1](https://arc4cfd.github.io/section1/outline/) of this course, the binary language is very efficient for programs and is not designed for humans to read. **Executables** for instance are written in binary code by the compiler, and contain the set of instructions a program has to execute.

2. **ASCII**: stands for American Standard Code for Information Interchange. It is a coded character set consisting of 128 7-bit characters. There are 32 control characters, 94 graphic characters, the space character, and the delete character. ASCII is a way of writing files that can be easily read by humans. A very common text file (.txt) is an ASCII file.

**Why does this matter in CFD and HPC?**

:::note[In general]
Binary style is faster for **read/write** since the machine does not have to convert to a human-readable format. The size of a binary output file is also smaller as compared to an ASCII file.
:::

Applying this reasoning to a CFD case:

1. For complex geometries and very large mesh files where the goal is to print the output for hundreds or thousands of snapshots, **binary** would be a better choice, as writing many data points and many snapshots can be done relatively instantaneously (compared to converting and writing thousands of ASCII files).

2. For relatively small cases on simple geometries, where the number of output files is not too large, writing ASCII files will not cause a significant performace hit, and one can open and manipulate single output files.

To Summarize:
<CustomAside icon="star" title="PROs" colour="green">
1. **ASCII**:
    - Suitable for small meshes and few snapshots. 
    - Can be visualized and edited using regular text editors.
2. **Binary**:
    - Smaller file size.
    - Faster to read/write
    - Suitable for large meshes and complex geometries.
</CustomAside>

<CustomAside icon="warning" title="CONs" colour="red">
1. **ASCII**:
    - Larger file size. 
2. **Binary**:
    - Cannot be read and edited by regular text editors.
</CustomAside>

:::tip[Tip]
Check the documentation of your CFD tool as you might be able to change how output files are written. In OpenFOAM, for instance, you can switch between the two write methods on the fly by modifying the `controlDict` entry while your case is running.
:::



