---
title: 'Running simulations on HPC'
---

import Box from '../../../components/Box.astro';
import Caption from '../../../components/Caption.astro';
import CustomAside from '../../../components/CustomAside.astro';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import CodeFetch from '../../../components/CodeFetch.astro';

:::note[Learning Objectives]
By the end of this section, you should be able to:
1. Organize and plan simulation files on the cluster
2. Run the simulation and optimize
3. Carry out a runtime analysis of flow parameters
:::

## Running CFD simulations on HPC: Overview

![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_workflow_PROC.png "Process simulations of the CFD simulation")



{/*[test](https://www.grc.nasa.gov/WWW/wind/valid/tutorial/spatconv.html)
[test](https://curiosityfluids.com/2016/09/09/establishing-grid-convergence/)*/}
{/*![HPCcompromise.](../../../assets/figs_section2/ARC4CFD_IO.svg "Competing aspects in setting up CFD simulations")*/}

We are now ready to start a large-scale CFD simulation. As mentioned in the previous sections, we have chosen a problem (the backward facing step, BFS) that has some very important flow features, but most importantly it's a case that has been extensively analyzed in the literature both numerically [Li et al. 1997](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/direct-numerical-simulation-of-turbulent-flow-over-a-backwardfacing-step/645D21758E8F74568008899C17B12ADD), and experimentally [Jovic and Driver 1994](https://ntrs.nasa.gov/api/citations/19940028784/downloads/19940028784.pdf). The purpose of this section is to standardize the workflow of **setting up**, **running**, and **monitoring** a large-scale CFD simulation on a remote HPC system. Buckle up!

## Organize simulation files on the cluster
The first and **most important** step is to well organize the simulation files on the cluster. During your career as a researcher you will probably end up running a **LARGE** number of numerical simulations, and a well planned bookkeeping can save you **A LOT** of time.

:::caution[Disclaimer]
1. Although, all examples and simulations will be carried out using the University of Waterloo cluster **Graham**, the same process will apply to any other Compute Ontario HPC systems.
2. The workflow presented here **is NOT** a *set in stone* rule, but rather a **suggestion** for any user of HPC resources.
:::

If you are following along with the course, you probably have already cloned all the example files and GitHub repository on the Cluster, if not please go back and check [section 2.1](https://arc4cfd.github.io/section2/part1/). The question now is: **how do we organize our simulation files?**

To answer this question, let's first figure out what options we have. Upon loggin into Graham, type:
```bash
[username@gra-login1 ~]$ diskusage_report
```
This command will check available disk space and the current disk utilization on our **personal** and **group** profiles. The output will look something like:
```bash
[username@gra-login1 ~]$ 
                             Description                 Space             # of files
                     /home (user username)              23G/50G             112/500k
                  /scratch (user username)            6633G/20T            14k/1000k
                 /project (group username)              0/2048k               0/1025
               /project (group def-piname)           292M/1000G             294/505k
            /project (group rrg-piname-ac)             49T/100T            112k/505k
```
Where `username` refers to your personal space, while `piname` to your group (or principal investigator) profile. `/home` has a capacity of 50GB and, as we mentioned earlier, is not the recommended place to store simulation results or data. `/home` is suitable for code development and version control. `/project (group rrg-piname-ac)` (in this case) is the largest directory on the cluster, it's linked to your principal investigator account, and is the place where **FINAL** data should be stored long-term. Finally, `/scratch` is the second largest directory, it's connected to the single user, and as mentioned in [section 2.1](https://arc4cfd.github.io/section2/part1/), it's the right place to **set up and run** your simulations.

:::danger[Final Warning]
Important files must be copied off `/scratch` regularly since they are not backed up and older files are subject to purging!
:::

Following this approach, one can clone from the course [GitHub repository](https://github.com/ARC4CFD/arc4cfd/tree/master) both the **OpenFOAM** and **SU2** BFS examples to the remote `home` into the respective case directory:

<Tabs group="tab-group">
    <TabItem label="/home/bfs_openfoam">
    ```bash
    [username@gra-login1 ~]$ ls
    README              case                movie.ogv           run.sh
    bfs_Umag.pvsm       mesh                run_jobscript.sh
    ```
    </TabItem>
    <TabItem label="/home/bfs_su2">
    ```bash
    [username@gra-login1 ~]$ ls
    Coarse                                       README
    Fine                                         Turbulent-flow-over-Backward-facing-step.pdf
    Intermediate                                 su2job.sh
    ```
    </TabItem>
</Tabs>

### Create the Run directory
Now that you have cloned the required tools in your `/home`, think of them as the **SOURCE** code of your CFD tools. Any modifications you want to do on the source should happen in `/home`, as it's your own personal space, and nothing will be purged from here. However:

:::caution[Caution]
`/home` is not the place **to run** your simulation. Depending on the size of your mesh, and the number of snapshots saved, you will reach the **quota limit** of 50GB very soon. 
:::

For this reson, once you are happy with the changes implemented on the source code, you should **copy** the code into a **Run directory** in `/scratch`. In this case no changes were required in the source files, therefore we can directly:
<Tabs group="tab-group">
    <TabItem label="/scratch/01_BFS_openFOAM">
    ```bash
    [username@gra-login1 ~/home/bfs_openfoam]$ cp -r * ./scratch/01_BFS_openFOAM
    ```
    </TabItem>
    <TabItem label="/scratch/02_BFS_SU2">
    ```bash
    [username@gra-login1 ~/home/bfs_su2]$ cp -r * ./scratch/02_BFS_SU2
    ```
    </TabItem>
</Tabs>

Don't underestimate the importance of **name convention** for files and directory. We reccommend to find your own preferred system, and be consistent with it. **At this point we are ready to setup the simulation.**


### Setting up simulation parameters
All the simulation files have already been prepared, and here we highlight only the most important steps to follow before running the simulation:

1. **Mesh size**: the first and most important choice is the **number of points**. We prepared the case so that students can run several **meshes**. Here, however, for semplicity's sake, we only follow the example of the coarsest mech of $237000$ grid points (named `bfs_200k_DDES`).

2. **Time step size**: the choice of the grid size directly impact the maximum time step size on e can use due to the CFL stability condition ([section 2.3](https://arc4cfd.github.io/section2/part3/)). For the chosen case (of $200000$ points) the CFL stability condition requires: $\Delta_t=10^{-4}$.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change deltaT to 1e-4;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

3. **Domain decomposition**: after performing the scaling test for a given mesh ([section 2.5](https://arc4cfd.github.io/section2/part5/)), we know how many processors we should use to optimize the CFD workflow. The user should modify the `numberOfSubdomains` entry in the `case/system/decomposeParDict` file. In this example we use 64 processors.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/decomposeParDict
    # change numberOfSubdomains to 64;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

4. **Simulation End Time**: especially when turbulence or geometric non-homogeneity are present, it's important to run the simulation long enough to let the flow properly develop in the computational domain. This phase is usually referred to as **the transient** phase. Once the flow is fully developed, a **steady state** is reached and the flow is referred to as **statistically stationary**. As mentioned in [section 2.3](https://arc4cfd.github.io/section2/part3/) estimating the time required to reach a steady-state is very difficult and is case-dependent. A reasonably good measure to get a rough estimate is the Large Eddy Turn-Over Time (LETOT). One LETOT is the time required for the largest eddy to leave the domain. **In this example we choose the End Time to correspond to $\approx 10$ LETOTs**.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change endTime to 0.5;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

5. **Snapshot time interval**: once again, when turbulence governs the physics of the flow we use statistical methods to process and analyze the results. For this reason once should collect a large sample of **flow realization** or snapshots in order to perform post-processing analysis and compute flow statistics. The interval between two successive snapshots should not be too large (as they will be uncorrelated) nor too small (as they will be too much correlated). One can always refer to the LETOT and choose a percetage of that as the sampling interval.
<details>
    <summary>OpenFOAM</summary>
    ```bash
    vim case/system/controlDict
    # change writeInterval to 20;
    ```
</details>
<details>
    <summary>SU2</summary>
    Example 2 content
</details>

After all flow and simulation paramters have been set, **we are now ready to run the simulation**. 

## Run a large-scale CFD simulation
As previously seen in (section 2.5) of this course when solving the 2D Poisson equation, the are 2 common ways of running large-scale simulations on the cluster: (i) interacive session by loggin into the compute nodes directly, and (ii) submitting a batch job to SLURM. For a very small simulation such as the 2D Poisson there not much difference (in terms of time) between the 2 methods, however when the simulation becomes more involved, disadvantages of the former and advantages of the latter become more evident.

### Running in interactive mode
Now that we copied the **Source** code from our `/home` directory into `/scratch`, our goal is to run 3 simulations starting from a coarse mesh ($\approx 200000$ points) to a fine mesh (of about $800000$ grid points). As we shall see in a later section, there is a reason behind this choice, but for now let's follow the steps towards running the simulation:

<Tabs group="tab-group">
    <TabItem label="OpenFOAM steps">
    1. **Allocate required HPC resources**:
    ```bash
    salloc -n 64 --time=10:00:0 --mem-per-cpu=3g --account=account-name
    ```
    2. **Create the sub-case directory** `bfs_200k_DDES` within the main case directory:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ cp -r * ./case/* ./bfs_200k_DDES
    ```
    3. **Generate mesh** from file using the `gmsh` utility:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gmsh mesh/bfs_200k.geo -3 -o mesh/bfs_200k.msh -format msh2
    ```
    4. **Convert mesh to OpenFOAM format** and **modify boundary file to reflect boundary conditions**:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gmshToFoam mesh/$mesh.msh -case $case
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ cp $case/constant/polyMesh/boundary $case/constant/polyMesh/boundary.old
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i '/physical/d' $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/wall_/,/startFace/{s/patch/wall/}" $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/top/,/startFace/{s/patch/symmetryPlane/}" $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/front/,/startFace/{s/patch/cyclic/}" $case/constant/polyMesh boundary                                        
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i "/back/,/startFace/{s/patch/cyclic/}" $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/front/,/}/{/startFace .*/a'"\\\tneighbourPatch  back;" -e '}' $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/back/,/}/{/startFace .*/a'"\\\tneighbourPatch  front;" -e '}' $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/cyclic/,/nFaces/{/type .*/a'"\\\tinGroups        1(cyclic);" -e '}' $case/constant/polyMesh/boundary
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sed -i -e '/wall_/,/}/{/type .*/a'"\\\tinGroups        1(wall);" -e '}' $case/constant/polyMesh/boundary
    ```

    5. **Start the simulation**:
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ cd ./bfs_200k_DDES
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ ./Allrun
    ```
    Where the `Allrun` script perfoms some very important opearions. Among them:
    ```bash
    cp -r 0.orig 0                # initialize the flow
    runApplication decomposePar   # decomposed the domain based on the # of procs.
    runParallel $(getApplication) # run the application in parallel
    runApplication reconstructPar # after simulation is done join processors into single file
    rm -rf processor*             # remove all the single processors directories
    ```
    After **step 5** is completed, you should see the simulation starting on the terminal:
    ```bash
    Running decomposePar on /home/ambrox/scratch/BFS_OpenFOAM/bfs_200k_DDES
    Running pimpleFoam in parallel on /home/ambrox/scratch/BFS_OpenFOAM/bfs_200k_DDES using 64 processes
    ```
    </TabItem>
    <TabItem label="SU2 steps">
    ```bash
    [username@gra-login1 ~/scratch/02_BFS_SU2]$ cp -r * ./case/* ./bfs_200k_DDES
    ```
    </TabItem>
</Tabs>

Here are some *PROs* and *CONs* of running a large scale simulation in interacive mode:
<CustomAside icon="star" title="PROs" colour="green">
1. Easier to set up.
2. Easier to run. 
</CustomAside>

<CustomAside icon="warning" title="CONs" colour="red">
1. **Not suited** for **LONG** jobs as the terminal window must stay open and the workstation on.
2. **Not suited** when running several simulations. 
</CustomAside>

### Submitting a batch script
In most of the cases, a CFD engineers, or a PhD student would need to run a large series of numerical simulation, might they involve different geometries, boundary conditions, mesh sizes etc. As you might have already guessed, running $N$ simulations in interactive mode not only becomes a tedious repetition of the same 5 steps (in the OpenFOAM case), but it also increase our chances to make mistakes along the way. 

A **very good** idea in HPC, when dealing with repeatitive tasks is the concept of **automation**. In this case, for instance, we could include Steps 1-5 in a single file `run.sh` to be run in interacive mode, or **even better** in a batch job script `run_jobscript.sh` to submit to the job scheduler. Both files are included in the GitHub repository:

<details>
    <summary>run.sh</summary>
    <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/openfoam/Tutorials/BFS_OpenFOAM/run.sh' lang='bash' meta="title='run.sh' " />
</details>
<details>
    <summary>run_jobscript.sh</summary>
    <CodeFetch rawURL='https://raw.githubusercontent.com/ARC4CFD/arc4cfd/openfoam/Tutorials/BFS_OpenFOAM/run_jobscript.sh' lang='bash' meta="title='run_jobscript.sh' " />
</details>

The command to submit the bath script would be simply:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sbatch run_jobscript.sh
Submitted batch job 26236582 
```

## Perform a runtime analysis of flow parameters
Once the job is submitted, we should make sure the simulation is running properly. This is done by typing the command:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ sq
JOBID     USER      ACCOUNT       NAME    ST  TIME_LEFT   NODES CPUS TRES_PER_N MIN_MEM NODELIST (REASON) 
26236582  username  def-piname  bfs_DDES   R   19:59:46     8    64        N/A    3G    gra[11005,11007,11010,11012-11016] (None)
```

The code is running, as expected, on 64 processors using 8 nodes. This check however, does not really tell us that **everything is going well** but only that the 64 processes have started and are working at something. The next step would be to check the **log file**. If you recall, with the command `mpirun pimpleFoam -parallel > log.pimpleFoam` in the **run_jobscript.sh** we asked the code to write any output to a log file called `log.pimpleFoam`. If you notice, this fill popped up into our case directory (`bfs_200k_DDES`) as soon as the simulation started. 

Depending on how far along in the simulation you are, the `log.pimplefoam` file might be quite long. To give you a quick run through of how it looks, let's visualize the beginning of it:

<details>
    <summary>see log.pimpleFoam</summary>
    ```bash
    [username@gra-login1 ~/scratch/01_BFS_openFOAM]$ vim log.pimpleFoam
    Starting time loop

    Courant Number mean: 0.37558691 max: 5.0866216
    Time = 0.0081

    PIMPLE: iteration 1
    DILUPBiCGStab:  Solving for Ux, Initial residual = 0.0543257, Final residual = 0.0030492453, No Iterations 1
    DILUPBiCGStab:  Solving for Uy, Initial residual = 0.019132138, Final residual = 0.0009267347, No Iterations 1
    DILUPBiCGStab:  Solving for Uz, Initial residual = 0.066712166, Final residual = 0.0049700607, No Iterations 1
    GAMG:  Solving for p, Initial residual = 0.98050352, Final residual = 0.021488268, No Iterations 2
    time step continuity errors : sum local = 0.00044866542, global = -1.3391353e-05, cumulative = -1.3391353e-05
    GAMG:  Solving for p, Initial residual = 0.14858043, Final residual = 8.6866034e-07, No Iterations 45
    time step continuity errors : sum local = 3.0582936e-08, global = -3.2758568e-09, cumulative = -1.3394628e-05
    PIMPLE: iteration 2
    DILUPBiCGStab:  Solving for Ux, Initial residual = 0.058760738, Final residual = 0.003117804, No Iterations 1
    DILUPBiCGStab:  Solving for Uy, Initial residual = 0.021669178, Final residual = 0.00078736235, No Iterations 1
    DILUPBiCGStab:  Solving for Uz, Initial residual = 0.12996742, Final residual = 0.010186982, No Iterations 1
    GAMG:  Solving for p, Initial residual = 0.59370611, Final residual = 0.014856221, No Iterations 2
    time step continuity errors : sum local = 0.00034748909, global = 1.1426898e-05, cumulative = -1.9677303e-06
    GAMG:  Solving for p, Initial residual = 0.30867957, Final residual = 9.1670175e-07, No Iterations 49
    time step continuity errors : sum local = 1.3500471e-08, global = 1.43852e-09, cumulative = -1.9662918e-06
    PIMPLE: iteration 3
    DILUPBiCGStab:  Solving for Ux, Initial residual = 0.015980338, Final residual = 0.0006615279, No Iterations 1
    DILUPBiCGStab:  Solving for Uy, Initial residual = 0.0087843757, Final residual = 0.00022240436, No Iterations 1
    DILUPBiCGStab:  Solving for Uz, Initial residual = 0.11693161, Final residual = 0.006951935, No Iterations 1
    GAMG:  Solving for p, Initial residual = 0.65043891, Final residual = 0.010075974, No Iterations 2
    time step continuity errors : sum local = 0.00010854745, global = 2.6004423e-06, cumulative = 6.3415051e-07
    GAMG:  Solving for p, Initial residual = 0.40140073, Final residual = 9.079401e-07, No Iterations 47
    time step continuity errors : sum local = 4.8308146e-09, global = 5.1590399e-10, cumulative = 6.3466642e-07
    ```
</details>

Important information to retain from the log file are:
1. **The time iteration** corresponds to the time integration of the equation of motion mentioned in [section 2.3](https://arc4cfd.github.io/section2/part3/).
2. **The CFL** or Courant number is displayed at every time stamp.
3. **Residuals** are shown at each iteration for all velocity components and pressure.
4. **Local and global** mass conservation is also printed at each iteration.

The 4 pieces of information are already incredibly useful to understand if the simulation is converging, divergin, if mass is globally conserved, or if there is a problem in the domain. 

The simulation will be running probably for several hours, and the ideal scenario is that every once in a while we check the behavior of the residuals and mass conservation. As you might guess staring at numbers on the screen is not the best approach and, once again, it is better to an automated mechanism to visualize residuals. This can be done using **gnuplot**, a command-line and GUI program that can generate two- and three-dimensional plots of functions, data, and data fits. Gnuplot is usually present by default in any UNIX system, however to make sure you have it in your profile on the cluster, you can type:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ gnuplot

	G N U P L O T
	Version 5.4 patchlevel 2    last modified 2021-06-01 

	Copyright (C) 1986-1993, 1998, 2004, 2007-2021
	Thomas Williams, Colin Kelley and many others

	gnuplot home:     http://www.gnuplot.info
	faq, bugs, etc:   type "help FAQ"
	immediate help:   type "help"  (plot window: hit 'h')

```
If you don't see the gnuplot welcome message, or if the terinal throws you an error, you can load the gnuplot module just like any other module:
```bash
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ module load gnuplot 
[username@gra-login1 ~/scratch/01_BFS_openFOAM]$ module save
```

## Perform a grid convergence study 
Explain the importance of a grid sensitivity study and how to carry it out [TO DO]

## SUGGESTION JPH: discuss I/O issues (binary vs ASCII, parallel output (e.g. OF) vs serial output (or parallel output + merge as done in SU2))
 ![HPCcompromise.](../../../assets/figs_section2/ ARC4CFD_IO.png "Estimate HPC costs of the CFD simulation")



## SUGGESTION JPH: Brief discussion on V\& V 
How can we validate and verify in the context of HPC. validate and verify on smaller problems, gain confidence and verify large scale test (avoid iterating as it's very expensive in HPC costs)