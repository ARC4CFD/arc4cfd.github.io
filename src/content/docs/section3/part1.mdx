---
title: 'Introduction to RDM'
---

:::note[Learning Objectives]
By the end of this section, you should be able to:
1. test
2. test 
:::



[test](https://carpentries-incubator.github.io/hpc-intro/18-responsibility/index.html)


This class introduces the concepts of Research Data Management and applies these concepts to the field of Computational Fluid Dynamics. 


## Underlying principle of research data management
Research data management (RDM) is concerned with the management of the entire lifecycle of data used or generated as part of a research project. RDM seeks to develop best practices to organize, structure, share, store and care for the scientific data. RDM stem from the understanding that 'publicly funded research data are a public good'   re ohmann_sharing_2017.  Therefore, RDM seeks to maximize the  value of research data for the greatest societal benefit. It should be noted that good data management is not a goal in itself \cite{wilkinson_fair_2016} but a means to better value the research data.


The field of RDM has emerged primarily from the need to manage sensitive (medical data) and/or expensive datasets (astrophysics) and driven by large-scale research endeavours. RDM has also developed parallel to the increasing push towards transparent and open science, which is further motivating the need for curated data. 

Within this context, the understanding of RDM principles is not as well developed within the CFD community. This is in part due to the nature of CFD workflows which is typically better suited for individual researchers or small research groups as opposed to large-research enterprise (such as in bio-informatics or astrophysics). 

Yet, the use of high performance computing with modern CFD tools generates lots of expensive dataset that are not fully valorized without modern RDM practices. We do note the following repositories:
JHTD [test](https://turbulence.pha.jhu.edu)
Turbase [test](https://turbase.cineca.it/init/routes/#/logging/welcome)


Why manage data?

   -  Facilicate research and avoid duplication of efforts
    -  Save data for later
   -  Share data for re-use
   -  Better dissimante the results (citation)
   -  open science (best practice)
   -  Meet funders requirements



## What is data in CFD?
The data underpins the scientific process, this is especially true in computational fluid dynamics.  In this context, 'data' is an umbrella term that includes:

 -  preliminary calculations,
 -  mesh,
 -  input files, configuration files 
 -  codes and solvers
 -  post-processed data
 -  visualization results
    input files
calculation files (estimation of y+)
post-processed
runtime data (statistics)
visualization
raw data
Tools
Solver (version number, git identifier)
post-processing scripts

 In the most simplistic terms, data lifecycle can be described by a three-step process, data is generated, valorized, and then archived. These steps will be further explored within this section.

Data management for CFD remains quite unique. 

Many large-scale projects, such as computational astronomy, have developed and established processes to assure the perenity of the data while maintaining tractability and free-access to large data bases. Within other communities, such computational fluid dynamics, the research endeavor does not entail large, multi-institutional research groups that demand large scale data management.

Yet, large-scale data exchanges are a key part of the modern scientific community. Much of the efforts over the past X years have focused on developing freely accessible plateforms to exchange large scale data. Although a curated dataset, from both experimental and computational fluid dynamics


Computational fluid dynamics remains unique among many modern disciplines relying on HPC. The constitutive equations are derived from classical Newtonian physics and can be written down as a system of partial differential equations. Yet, the there is no generalizeable closed form solution, especially in turbulent flow. This is especially problematic given the multi-scale nature of turbulence. Modern efforts, especially for scale resolving simulations, are pushing 
High fidelity data must be generated, yet it is not apriori clear what data is relevant to understanding the problem.







ref selent_management_2020 : paper explains the different stages of data in CFD











## TRUST and FAIR

The FAIR principles  wilkinson_fair_2016 ref

  -  Findable:
  -  Accessible:
  -  Interoperable: 
  -  Reusable:


The TRUST principles underpin ref lin_trust_2020
1. Transparency:
2. Responsibility:
3. User Focus:
4. Sustainability:
5. Technology.




FAIR
[test](Findable, Accessible, Interoperable, and Reusable)
[test](https://force11.org/info/the-fair-data-principles/)





## Ressources
[test](https://scienceeurope.org/media/4brkxxe5/se_rdm_practical_guide_extended_final.pdf)





## Determining the principles of RDM in CFD

We should aim for a holistic data management (solver/tools/etc)

What are the challenges: infrastructure and tools, capacity/skills, ethical (less important for CFD), culture/incentive




### Learning from other fields 
AI
See this course: [test](https://thodrek.github.io/CS839_spring18/)
[test](https://fullstackdeeplearning.com/spring2021/lecture-8/)



"The resulting data ecosystem, therefore, appears to be moving away from centralization, is becoming more diverse, and less integrated, thereby exacerbating the discovery and re-usability problem for both human and computational stakeholders." \cite{wilkinson_fair_2016}

Important discussion [test](https://towardsdatascience.com/data-entropy-more-data-more-problems-fa889a9dd0ec)


Workshop in TUM
[test](https://zenodo.org/records/7785532#.ZCw4h85ByUk)

[test](https://nfdi4ing.de)

[test](https://www.youtube.com/watch?v=3sDhQRIYUmA)
